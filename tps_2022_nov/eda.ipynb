{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Kaggle Tabular dataset competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Table of contents\n",
    "# 1. Importing libraries\n",
    "# 2. Loading data\n",
    "# 3. Data exploration\n",
    "# 4. Data cleaning\n",
    "# 5. Feature engineering \n",
    "# 6. Model training\n",
    "# 7. Model evaluation\n",
    "# 8. Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.feature_selection import SelectFdr\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import mlflow\n",
    "import pickle\n",
    "import mlflow.xgboost\n",
    "import mlflow.sklearn\n",
    "\n",
    "#turn of warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 0.2 Load the data to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:50<00:00, 98.95it/s] \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load in all data used for train and test\n",
    "\"\"\"\n",
    "# read in data from train and test csv files\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "#print out the first 5 rows of the train_labels dataframe\n",
    "\n",
    "\n",
    "#read all files in folder submission files as dataframes, and concatenate them into one dataframe using tqdm for progress bar\n",
    "submission_files = os.listdir('submission_files')\n",
    "submission_files = [file for file in submission_files if file.endswith('.csv')]\n",
    "submission_dfs = []\n",
    "for file in tqdm.tqdm(submission_files):\n",
    "    submission_dfs.append(pd.read_csv('submission_files/'+file)[\"pred\"])\n",
    "\n",
    "submission_df2 = pd.concat(submission_dfs, axis=1)\n",
    "\n",
    "\n",
    "#make colnames filename:\n",
    "columns = [i for i in submission_files]\n",
    "#remove .csv from columns names:\n",
    "columns = [i[:-4] for i in columns]\n",
    "submission_df2.columns = columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Make a validation set and a traintest set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# differentiate the dataset into the one we have lables for and the one we do not have lables for\n",
    "#sort the submission_df2 dataframe by column name, using quicksort:\n",
    "submission_df2.sort_index(axis=1, kind='quicksort')\n",
    "\n",
    "\n",
    "traintest_df = submission_df2.iloc[:int(len(submission_df2)/2)]\n",
    "validation_df = submission_df2.iloc[int(len(submission_df2)/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Start looking at different data cleaning techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Flip predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are:  31 worse than 50% predictions in the traintest_df, these are:  ['0.6865464623', '0.7407076677', '0.7038139555', '0.6338206806', '0.6362728614', '0.6931174765', '0.6286579774', '0.6432872696', '0.7129056097', '0.6675187404', '0.6665224750', '0.7253196911', '0.6932630537', '0.6799983348', '0.6870215891', '0.6794230620', '0.6998693893', '0.6982037018', '0.6457425277', '0.6926167433', '0.6678652455', '0.7481503216', '0.6997714112', '0.6921813451', '0.6794001635', '0.7141305285', '0.6526257846', '0.6863963491', '0.6933054832', '0.6868949322', '0.6587320359']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# flip the predictions in columns where average prediction is less than 0.5:\n",
    "list_of_filpped_columns = []\n",
    "for col in traintest_df.columns[:-1]:\n",
    "    if traintest_df[col].mean() < 0.5:\n",
    "        list_of_filpped_columns.append(col)\n",
    "        traintest_df[col] = 1 - traintest_df[col]\n",
    "\n",
    "print(\"There are: \", len(list_of_filpped_columns), \"worse than 50% predictions in the traintest_df, these are: \", list_of_filpped_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 remove erronous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There are: ', 107, 'columns with entries lower than 0 or higher than 1, these are: ', ['0.7439608123', '0.6709092823', '0.6957237605', '0.6813468284', '0.6816092339', '0.7069356577', '0.6917084219', '0.6917864949', '0.6460217501', '0.7512639042', '0.7519100517', '0.6926922554', '0.6907530914', '0.6802939843', '0.6634226966', '0.6822568758', '0.6855649936', '0.7035478918', '0.6981892492', '0.6768337040', '0.6828592913', '0.7551167673', '0.6831608458', '0.6896882783', '0.6882463725', '0.6818171939', '0.6825756120', '0.6991497122', '0.6910786227', '0.6838089987', '0.7001360164', '0.6775430865', '0.6973401911', '0.6887896671', '0.6887113415', '0.6838614356', '0.7051369296', '0.6983372156', '0.7135795615', '0.6969724370', '0.7362435667', '0.7361729002', '0.7098719584', '0.7496569929', '0.6935553446', '0.6846598508', '0.6828278513', '0.6970620517', '0.7010914098', '0.7376874288', '0.6874413617', '0.6853494192', '0.7078929135', '0.7324623956', '0.7160613710', '0.7370613359', '0.6843211995', '0.6920621575', '0.6870081097', '0.7010942574', '0.6669113699', '0.6864963083', '0.6887326081', '0.6904362846', '0.6840480588', '0.6814163062', '0.7063319482', '0.7363893320', '0.6946294697', '0.7447146349', '0.6995667247', '0.6873111850', '0.6918517657', '0.6915831030', '0.6757779307', '0.6984484815', '0.6816836062', '0.6926167433', '0.6839808582', '0.6678652455', '0.6952088416', '0.7288642994', '0.6878439542', '0.6779491802', '0.6969992922', '0.6805411309', '0.6888856381', '0.6835417824', '0.6852679092', '0.6926487768', '0.6721541591', '0.6935271564', '0.6975943436', '0.6889711448', '0.6911531304', '0.6775478426', '0.6829122493', '0.6849822379', '0.6828159094', '0.7133361130', '0.7353665174', '0.6881883853', '0.6372037651', '0.6888425912', '0.6982536185', '0.6995800303', '0.6607013428'])\n"
     ]
    }
   ],
   "source": [
    "#remove columns with entries lower than 0 or higher than 1:\n",
    "\n",
    "list_of_bad_columns = []\n",
    "for col in traintest_df.columns:\n",
    "    if traintest_df[col].min() < 0 or traintest_df[col].max() > 1:\n",
    "        list_of_bad_columns.append(col)\n",
    "        traintest_df = traintest_df.drop(col, axis=1) #drop the column from the train and test dataframe\n",
    "        validation_df = validation_df.drop(col, axis=1) #remove the same column from the validation_df\n",
    "\n",
    "print((\"There are: \", len(list_of_bad_columns), \"columns with entries lower than 0 or higher than 1, these are: \", list_of_bad_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Do a PCA-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 4893 features, but PCA is expecting 4894 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [189], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m traintest_df_PCA \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data \u001b[39m=\u001b[39m principalComponents, columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mprincipal component \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, N\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[1;32m      7\u001b[0m \u001b[39m#also fit validation data:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#sort validation_df columns:\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m validation_df_PCA_pre_transform \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mtransform(validation_df)\n\u001b[1;32m     10\u001b[0m validation_df_PCA \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data \u001b[39m=\u001b[39m validation_df_PCA_pre_transform, columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mprincipal component \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, N\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[1;32m     13\u001b[0m \u001b[39m#plot the explained variance ratio\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/decomposition/_base.py:120\u001b[0m, in \u001b[0;36m_BasePCA.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\"\"\"Apply dimensionality reduction to X.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[39mX is projected on the first principal components previously extracted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39m    is the number of samples and `n_components` is the number of the components.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     X \u001b[39m=\u001b[39m X \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 4893 features, but PCA is expecting 4894 features as input."
     ]
    }
   ],
   "source": [
    "# Do PCA analysis to see if we can reduce the number of features:\n",
    "N = 800\n",
    "pca = PCA(n_components=N)\n",
    "principalComponents = pca.fit_transform(traintest_df)\n",
    "traintest_df_PCA = pd.DataFrame(data = principalComponents, columns = ['principal component ' + str(i) for i in range(1, N+1)])\n",
    "\n",
    "#also fit validation data:\n",
    "#sort validation_df columns:\n",
    "validation_df_PCA_pre_transform = pca.transform(validation_df)\n",
    "validation_df_PCA = pd.DataFrame(data = validation_df_PCA_pre_transform, columns = ['principal component ' + str(i) for i in range(1, N+1)])\n",
    "\n",
    "\n",
    "#plot the explained variance ratio\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.show()\n",
    "\n",
    "#Also plot the explained variance ratio for the first 100 components\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)[:100])\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Prepare the dataset as a dataframe for the model with preds\n",
    "traintest_df_PCA = pd.concat([traintest_df_PCA, train_labels[\"label\"]], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Test different feature reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Specs       Score\n",
      "581   0.6371788217  453.746112\n",
      "4545  0.6223807245  442.933262\n",
      "3113  0.6513124137  430.582800\n",
      "813   0.6335286032  424.206245\n",
      "3059  0.6492627054  423.592043\n",
      "...            ...         ...\n",
      "2002  0.6664363848  330.387196\n",
      "3575  0.6703830405  330.300180\n",
      "3046  0.6677664204  330.248736\n",
      "4689  0.6650874454  330.126639\n",
      "3028  0.6656178113  329.966587\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Try using featureselection to reduce the number of features:\n",
    "\n",
    "\n",
    "#apply SelectKBest class to extract top 500 best features:\n",
    "K = 200\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=K)\n",
    "\n",
    "fit = bestfeatures.fit(traintest_df, train_labels[\"label\"])\n",
    "\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(traintest_df.columns)\n",
    "\n",
    "#concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "\n",
    "print(featureScores.nlargest(K,'Score'))  #print K best features\n",
    "\n",
    "#make a list of the 500 best features\n",
    "list_of_K_best_features = featureScores.nlargest(K,'Score')[\"Specs\"].tolist()\n",
    "\n",
    "#make a dataframe with only the 500 best features\n",
    "traintest_df_chi2 = traintest_df[list_of_K_best_features]\n",
    "#also make validation_df_500:\n",
    "validation_df_chi2 = validation_df[list_of_K_best_features]\n",
    "#add labels to the traintest_df_chi2 dataframe:\n",
    "traintest_df_chi2 = pd.concat([traintest_df_chi2, train_labels[\"label\"]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Specs        Score\n",
      "3059  0.6492627054  6863.201237\n",
      "3706  0.6501134385  6811.543011\n",
      "429   0.6454375051  6802.921218\n",
      "4341  0.6453694232  6793.860932\n",
      "869   0.6483090984  6790.168636\n",
      "...            ...          ...\n",
      "4318  0.6649683412  6073.224531\n",
      "1140  0.6646072770  6073.109233\n",
      "521   0.6645763819  6071.314455\n",
      "1079  0.6761696564  6068.904955\n",
      "3038  0.6756269427  6064.641131\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# instead of chi2 we can also use f_classif, mutual_info_classif, and SelectFromModel\n",
    "# use SelectKBest with f_classif:\n",
    "K = 200\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=K)\n",
    "#make a list of the K best features\n",
    "\n",
    "fit = bestfeatures.fit(traintest_df, train_labels[\"label\"])\n",
    "\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(traintest_df.columns)\n",
    "\n",
    "#concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "list_of_K_best_features= featureScores.nlargest(K,'Score')[\"Specs\"].tolist()\n",
    "\n",
    "\n",
    "print(featureScores.nlargest(K,'Score'))  #print 500 best features\n",
    "\n",
    "#make a dataframe with only the K best features\n",
    "traintest_df_f_classif = traintest_df[list_of_K_best_features]\n",
    "#also make validation_df:\n",
    "validation_df_f_classif = validation_df[list_of_K_best_features]\n",
    "#add labels to the traintest_df_f_classif dataframe:\n",
    "traintest_df_f_classif = pd.concat([traintest_df_f_classif, train_labels[\"label\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Do the model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add label to traintest_df:\n",
    "traintest_df = pd.concat([traintest_df, train_labels[\"label\"]], axis=1)\n",
    "\n",
    "\n",
    "# Current dataset verisons:\n",
    "traintest_df_clean = traintest_df\n",
    "validation_df_clean = validation_df\n",
    "#PCA dataframes:\n",
    "traintest_df_PCA = principalDf\n",
    "validation_df_PCA = validation_df_PCA\n",
    "#feature reduced datasets:\n",
    "traintest_df_chi2 = traintest_df_chi2\n",
    "validation_df_chi2 = validation_df_chi2\n",
    "\n",
    "traintest_df_f_classif = traintest_df_f_classif\n",
    "validation_df_f_classif = validation_df_f_classif\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# \n",
    "X, y  = traintest_df_f_classif.drop(\"label\", axis=1), traintest_df_f_classif[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 200) X_train shape\n",
      "(4000, 200) X_test shape\n",
      "(16000,)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "alpha\t0.19940462245879345\n",
    "colsample_bylevel\t0.6082032831689801\n",
    "colsample_bytree\t0.5026761553080533\n",
    "custom_metric\tNone\n",
    "early_stopping_rounds\t50\n",
    "eta\t0.01834603996998217\n",
    "eval_metric\t['error', 'auc', 'logloss']\n",
    "gamma\t0.01897331639253623\n",
    "lambda\t0.14867997340472994\n",
    "max_depth\t5\n",
    "maximize\tNone\n",
    "min_child_weight\t0.5347175461839557\n",
    "num_boost_round\t1000\n",
    "objective\tbinary:logistic\n",
    "\"\"\"\n",
    "#set params from above:\n",
    "params_from_hyperopt2 = {\n",
    "    'alpha': 0.19940462245879345,\n",
    "    'colsample_bylevel': 0.6082032831689801,\n",
    "    'colsample_bytree': 0.5026761553080533,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'eta': 0.01834603996998217,\n",
    "    'gamma': 0.01897331639253623,\n",
    "    'lambda': 0.14867997340472994,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 0.5347175461839557,\n",
    "    'num_boost_round': 1000,\n",
    "    'objective': 'binary:logistic'\n",
    "    }\n",
    "\n",
    "#set params from above:\n",
    "params_from_hyperopt = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['error', 'auc', 'logloss'],\n",
    "    'eta': 0.09021011892819718,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 0.7776461453397946,\n",
    "    'subsample': 0.7649830775674538,\n",
    "    'colsample_bytree': 0.4806978784720332,\n",
    "    'colsample_bylevel': 0.8876867721023922,\n",
    "    'alpha': 0.0918962468520413,\n",
    "    'lambda': 0.4786418563322011,\n",
    "    'gamma': 0.0038307274657378857,\n",
    "    'seed': 41\n",
    "}\n",
    "\n",
    "params_default = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'min_child_weight': 1.2,\n",
    "    #set small alpha:\n",
    "    'alpha': 0.01,\n",
    "    'lambda': 1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(X_train.shape, \"X_train shape\")\n",
    "print(X_test.shape, \"X_test shape\")\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train )\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:34:32] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-cpython-38/xgboost/src/learner.cc:767: \n",
      "Parameters: { \"early_stopping_rounds\", \"num_boost_round\" } are not used.\n",
      "\n",
      "[0]\ttrain-logloss:0.68771\ttest-logloss:0.68790\n",
      "[50]\ttrain-logloss:0.55125\ttest-logloss:0.55960\n",
      "[100]\ttrain-logloss:0.51213\ttest-logloss:0.52866\n",
      "[150]\ttrain-logloss:0.49548\ttest-logloss:0.52010\n",
      "[200]\ttrain-logloss:0.48625\ttest-logloss:0.51786\n",
      "[250]\ttrain-logloss:0.48013\ttest-logloss:0.51719\n",
      "[300]\ttrain-logloss:0.47432\ttest-logloss:0.51680\n",
      "[350]\ttrain-logloss:0.46854\ttest-logloss:0.51679\n",
      "[400]\ttrain-logloss:0.46285\ttest-logloss:0.51663\n",
      "[450]\ttrain-logloss:0.45320\ttest-logloss:0.51656\n",
      "[500]\ttrain-logloss:0.44334\ttest-logloss:0.51662\n",
      "[550]\ttrain-logloss:0.43496\ttest-logloss:0.51697\n",
      "[552]\ttrain-logloss:0.43457\ttest-logloss:0.51700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/06 12:34:52 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "2022/11/06 12:35:08 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 75def85e3e3d4976a40b4fc296bdbe08. Failed operations: [RestException(\"INVALID_PARAMETER_VALUE: Duplicate parameter keys have been submitted: [\\'num_boost_round\\', \\'early_stopping_rounds\\']. Please ensure the request contains only one param value per param key.\")]')]\n"
     ]
    }
   ],
   "source": [
    "# print output for each 50th iteration:\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "mlflow.end_run()\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"subset_models\")\n",
    "with mlflow.start_run():\n",
    "    evals_result = {}\n",
    "    model = xgb.train(params_from_hyperopt2, dtrain, num_boost_round=1000, evals=watchlist,evals_result = evals_result, verbose_eval=50, early_stopping_rounds=100)\n",
    "    y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5170024261624203\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy using round:\n",
    "y_pred_round = np.round(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred_round)\n",
    "\n",
    "\n",
    "logloss_score = log_loss(y_test, y_pred)\n",
    "print(logloss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot logloss over time for train and test sets:\n",
    "plt.plot(evals_result['train']['logloss'], label='train logloss')\n",
    "plt.plot(evals_result['test']['logloss'], label='test logloss')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_iteration = model.best_iteration\n",
    "print(best_iteration)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m98612/venv/fradrag2021/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n",
      "/var/folders/9_/syprsqwd263_tvl61_yf9pwm0000gn/T/ipykernel_63042/1632241534.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_df[\"pred\"]= model.predict(xgb.DMatrix(test_df), ntree_limit=best_iteration)\n",
      "/var/folders/9_/syprsqwd263_tvl61_yf9pwm0000gn/T/ipykernel_63042/1632241534.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_df[\"id\"] = validation_df.index\n"
     ]
    }
   ],
   "source": [
    "#Prep the submission format:\n",
    "validation_df[\"pred\"]= model.predict(xgb.DMatrix(test_df), ntree_limit=best_iteration)\n",
    "validation_df[\"id\"] = validation_df.index\n",
    "\n",
    "#save to file:\n",
    "validation_df[[\"id\", \"pred\"]].to_csv(\"results/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test of Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "search_space = {\n",
    "    \"objective\": \"binary:logistic\",  # objective\n",
    "    \"eta\": hp.loguniform(\"learning_rate\", -4, -1.2),  # learning rate\n",
    "    \"min_child_weight\": hp.loguniform(\n",
    "        \"min_child_weight\", -1, 3\n",
    "    ),  # minimum sum of weights of all observations required in child.\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 4, 20, 1)),\n",
    "    \"subsample\": hp.loguniform(\n",
    "        \"subsample\", -0.91, 0\n",
    "    ),  # fraction of observations to be randomly samples for each tree.\n",
    "    \"colsample_bytree\": hp.loguniform(\n",
    "        \"colsample_bytree\", -0.91, 0\n",
    "    ),  # fraction of columns to be randomly sampled for each tree.\n",
    "    \"colsample_bylevel\": hp.loguniform(\n",
    "        \"colsample_bylevel\", -0.91, 0\n",
    "    ),  # subsample ratio of columns for each split, in each level.\n",
    "    \"lambda\": hp.loguniform(\n",
    "        \"lambda\", -4, 0\n",
    "    ),  # L2 regularization term on weights (Ridge)\n",
    "    \"alpha\": hp.loguniform(\n",
    "        \"alpha\", -4, -1.2\n",
    "    ),  # L1 regularization term on weight (Lasso)\n",
    "    \"gamma\": hp.loguniform(\n",
    "        \"gamma\", -6, 2.7\n",
    "    ),  # minimum loss reduction required to make a split\n",
    "    \"seed\": 41,\n",
    "}\n",
    "\n",
    "\n",
    "def objective(search_space):\n",
    "    \"\"\"\n",
    "    Encapsulated objective-function such that hyperopt function can reach input-data in objective function\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(X_train.copy(), label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test.copy(), label=y_test)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "    verbose_eval = 50\n",
    "    with mlflow.start_run(nested=True):\n",
    "        search_space[\"eval_metric\"] = [\"error\", \"auc\", \"logloss\"]\n",
    "        num_round = 1000\n",
    "        evals_result = {}\n",
    "        num_features = 80\n",
    "        bst = xgb.train(\n",
    "            search_space,\n",
    "            dtrain,\n",
    "            num_round,\n",
    "            evals=watchlist,\n",
    "            evals_result=evals_result,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=verbose_eval,\n",
    "        )\n",
    "        print(f\"Stopping after {len(evals_result['train']['error'])} rounds\")\n",
    "\n",
    "\n",
    "        min_logloss_error= np.min(evals_result[\"test\"][\"logloss\"])\n",
    "        mlflow.xgboost.log_model(bst, artifact_path=\"model\")\n",
    "\n",
    "        #use cross validation to get the best iteration:\n",
    "        # cv_results = xgb.cv(\n",
    "        #    search_space,\n",
    "        #   dtrain,\n",
    "        #   num_boost_round=num_round,\n",
    "        #  nfold=5,\n",
    "        # early_stopping_rounds=50,\n",
    "        # verbose_eval=verbose_eval,\n",
    "        # show_stdv=False,\n",
    "        # )\n",
    "        # print(cv_results)\n",
    "        # min_logloss_error = cv_results[\"test-logloss-mean\"].min()\n",
    "        # print(f\"Best logloss: {min_logloss_error}\")\n",
    "        # best_iteration = cv_results[\"test-logloss-mean\"].idxmin()\n",
    "        # print(f\"Best iteration: {best_iteration}\")\n",
    "        # mlflow.log_metric(\"best_iteration\", best_iteration)\n",
    "        # mlflow.log_metric(\"best_logloss\", min_logloss_error)\n",
    "        #  \n",
    "\n",
    "        return {\n",
    "            \"status\": STATUS_OK,\n",
    "            \"loss\": min_logloss_error,\n",
    "            \"booster\": bst.attributes(),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/04 12:15:33 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of xgboost. If you encounter errors during autologging, try upgrading / downgrading xgboost to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.xgboost.autolog(\n",
    "    log_input_examples=False, log_model_signatures=True, log_models=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/04 20:54:50 INFO mlflow.tracking.fluent: Experiment with name 'hyperopt_test_5_subset' does not exist. Creating a new experiment.\n",
      "2022/11/04 20:54:50 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of xgboost. If you encounter errors during autologging, try upgrading / downgrading xgboost to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.18094\ttrain-auc:0.88703\ttrain-logloss:0.62962\ttest-error:0.35100\ttest-auc:0.71579\ttest-logloss:0.66473\n",
      "[50]\ttrain-error:0.00106\ttrain-auc:0.99996\ttrain-logloss:0.12800\ttest-error:0.29150\ttest-auc:0.79569\ttest-logloss:0.56648\n",
      "[70]\ttrain-error:0.00019\ttrain-auc:1.00000\ttrain-logloss:0.08237\ttest-error:0.29325\ttest-auc:0.79541\ttest-logloss:0.58178\n",
      "Stopping after 72 rounds                               \n",
      "[0]\ttrain-error:0.25731\ttrain-auc:0.82848\ttrain-logloss:0.67569\ttest-error:0.31250\ttest-auc:0.75825\ttest-logloss:0.67961\n",
      "[50]\ttrain-error:0.12156\ttrain-auc:0.94935\ttrain-logloss:0.38183\ttest-error:0.28500\ttest-auc:0.80381\ttest-logloss:0.53001\n",
      "[100]\ttrain-error:0.09300\ttrain-auc:0.96871\ttrain-logloss:0.33269\ttest-error:0.28375\ttest-auc:0.80430\ttest-logloss:0.52875\n",
      "[128]\ttrain-error:0.07575\ttrain-auc:0.97763\ttrain-logloss:0.31297\ttest-error:0.28350\ttest-auc:0.80399\ttest-logloss:0.53015\n",
      "Stopping after 130 rounds                                                        \n",
      "[0]\ttrain-error:0.24300\ttrain-auc:0.84288\ttrain-logloss:0.60769\ttest-error:0.35125\ttest-auc:0.73309\ttest-logloss:0.64400\n",
      "[50]\ttrain-error:0.01519\ttrain-auc:0.99847\ttrain-logloss:0.15834\ttest-error:0.30175\ttest-auc:0.77798\ttest-logloss:0.61425\n",
      "[62]\ttrain-error:0.00525\ttrain-auc:0.99942\ttrain-logloss:0.12317\ttest-error:0.30725\ttest-auc:0.77742\ttest-logloss:0.63176\n",
      "Stopping after 63 rounds                                                         \n",
      "[0]\ttrain-error:0.27469\ttrain-auc:0.80638\ttrain-logloss:0.64312\ttest-error:0.27825\ttest-auc:0.80391\ttest-logloss:0.64353\n",
      "[50]\ttrain-error:0.26269\ttrain-auc:0.83342\ttrain-logloss:0.49217\ttest-error:0.27175\ttest-auc:0.81232\ttest-logloss:0.51937\n",
      "[99]\ttrain-error:0.25350\ttrain-auc:0.84576\ttrain-logloss:0.47762\ttest-error:0.27000\ttest-auc:0.80984\ttest-logloss:0.52255\n",
      "Stopping after 100 rounds                                                        \n",
      "[0]\ttrain-error:0.27169\ttrain-auc:0.81037\ttrain-logloss:0.66992\ttest-error:0.29400\ttest-auc:0.79107\ttest-logloss:0.67147\n",
      "[50]\ttrain-error:0.24669\ttrain-auc:0.85252\ttrain-logloss:0.47416\ttest-error:0.27150\ttest-auc:0.81306\ttest-logloss:0.51911\n",
      "[100]\ttrain-error:0.22712\ttrain-auc:0.87694\ttrain-logloss:0.44514\ttest-error:0.27450\ttest-auc:0.81062\ttest-logloss:0.52118\n",
      "Stopping after 101 rounds                                                        \n",
      "[0]\ttrain-error:0.27244\ttrain-auc:0.81160\ttrain-logloss:0.68750\ttest-error:0.27475\ttest-auc:0.80504\ttest-logloss:0.68760\n",
      "[50]\ttrain-error:0.26081\ttrain-auc:0.82852\ttrain-logloss:0.54979\ttest-error:0.26925\ttest-auc:0.81292\ttest-logloss:0.55801\n",
      "[100]\ttrain-error:0.25912\ttrain-auc:0.83134\ttrain-logloss:0.51464\ttest-error:0.26925\ttest-auc:0.81338\ttest-logloss:0.52896\n",
      "[150]\ttrain-error:0.25806\ttrain-auc:0.83447\ttrain-logloss:0.50188\ttest-error:0.26800\ttest-auc:0.81379\ttest-logloss:0.52111\n",
      "[200]\ttrain-error:0.25650\ttrain-auc:0.83678\ttrain-logloss:0.49645\ttest-error:0.26725\ttest-auc:0.81353\ttest-logloss:0.51907\n",
      "[250]\ttrain-error:0.25550\ttrain-auc:0.83788\ttrain-logloss:0.49436\ttest-error:0.26750\ttest-auc:0.81364\ttest-logloss:0.51841\n",
      "[300]\ttrain-error:0.25525\ttrain-auc:0.83892\ttrain-logloss:0.49234\ttest-error:0.26825\ttest-auc:0.81372\ttest-logloss:0.51794\n",
      "[350]\ttrain-error:0.25475\ttrain-auc:0.84016\ttrain-logloss:0.49049\ttest-error:0.26825\ttest-auc:0.81380\ttest-logloss:0.51767\n",
      "[400]\ttrain-error:0.25419\ttrain-auc:0.84096\ttrain-logloss:0.48913\ttest-error:0.26775\ttest-auc:0.81368\ttest-logloss:0.51756\n",
      "[450]\ttrain-error:0.25425\ttrain-auc:0.84179\ttrain-logloss:0.48787\ttest-error:0.26775\ttest-auc:0.81373\ttest-logloss:0.51744\n",
      "[483]\ttrain-error:0.25369\ttrain-auc:0.84229\ttrain-logloss:0.48713\ttest-error:0.26800\ttest-auc:0.81366\ttest-logloss:0.51750\n",
      "Stopping after 485 rounds                                                        \n",
      "[0]\ttrain-error:0.25019\ttrain-auc:0.83084\ttrain-logloss:0.66857\ttest-error:0.32775\ttest-auc:0.74722\ttest-logloss:0.67557\n",
      "[50]\ttrain-error:0.08519\ttrain-auc:0.97198\ttrain-logloss:0.32930\ttest-error:0.28125\ttest-auc:0.80093\ttest-logloss:0.53281\n",
      "[100]\ttrain-error:0.04419\ttrain-auc:0.99036\ttrain-logloss:0.26696\ttest-error:0.28425\ttest-auc:0.80138\ttest-logloss:0.53676\n",
      "[102]\ttrain-error:0.04263\ttrain-auc:0.99111\ttrain-logloss:0.26294\ttest-error:0.28550\ttest-auc:0.80148\ttest-logloss:0.53681\n",
      "Stopping after 104 rounds                                                        \n",
      "[0]\ttrain-error:0.25369\ttrain-auc:0.82354\ttrain-logloss:0.68611\ttest-error:0.30850\ttest-auc:0.76646\ttest-logloss:0.68741\n",
      "[50]\ttrain-error:0.21413\ttrain-auc:0.88686\ttrain-logloss:0.50836\ttest-error:0.27225\ttest-auc:0.81105\ttest-logloss:0.55591\n",
      "[100]\ttrain-error:0.19831\ttrain-auc:0.90250\ttrain-logloss:0.44566\ttest-error:0.27150\ttest-auc:0.81175\ttest-logloss:0.52747\n",
      "[150]\ttrain-error:0.18050\ttrain-auc:0.91614\ttrain-logloss:0.41441\ttest-error:0.27150\ttest-auc:0.81175\ttest-logloss:0.52144\n",
      "[200]\ttrain-error:0.17125\ttrain-auc:0.92572\ttrain-logloss:0.39656\ttest-error:0.27075\ttest-auc:0.81185\ttest-logloss:0.52041\n",
      "[221]\ttrain-error:0.16906\ttrain-auc:0.92948\ttrain-logloss:0.39070\ttest-error:0.27225\ttest-auc:0.81186\ttest-logloss:0.52031\n",
      "Stopping after 222 rounds                                                        \n",
      "[0]\ttrain-error:0.21500\ttrain-auc:0.86422\ttrain-logloss:0.68297\ttest-error:0.33000\ttest-auc:0.72832\ttest-logloss:0.68708\n",
      "[50]\ttrain-error:0.04769\ttrain-auc:0.99187\ttrain-logloss:0.39771\ttest-error:0.28200\ttest-auc:0.80510\ttest-logloss:0.55304\n",
      "[100]\ttrain-error:0.01887\ttrain-auc:0.99824\ttrain-logloss:0.28977\ttest-error:0.27725\ttest-auc:0.80857\ttest-logloss:0.52738\n",
      "[150]\ttrain-error:0.00919\ttrain-auc:0.99944\ttrain-logloss:0.24092\ttest-error:0.27250\ttest-auc:0.81001\ttest-logloss:0.52402\n",
      "[197]\ttrain-error:0.00481\ttrain-auc:0.99977\ttrain-logloss:0.22190\ttest-error:0.27275\ttest-auc:0.81002\ttest-logloss:0.52522\n",
      "Stopping after 199 rounds                                                        \n",
      "[0]\ttrain-error:0.24844\ttrain-auc:0.82470\ttrain-logloss:0.68529\ttest-error:0.34800\ttest-auc:0.72728\ttest-logloss:0.68806\n",
      "[50]\ttrain-error:0.11525\ttrain-auc:0.95379\ttrain-logloss:0.46371\ttest-error:0.27975\ttest-auc:0.80692\ttest-logloss:0.56079\n",
      "[100]\ttrain-error:0.09731\ttrain-auc:0.96491\ttrain-logloss:0.38056\ttest-error:0.28125\ttest-auc:0.80724\ttest-logloss:0.53276\n",
      "[150]\ttrain-error:0.08806\ttrain-auc:0.97094\ttrain-logloss:0.34469\ttest-error:0.28025\ttest-auc:0.80778\ttest-logloss:0.52601\n",
      "[200]\ttrain-error:0.08200\ttrain-auc:0.97496\ttrain-logloss:0.32808\ttest-error:0.28075\ttest-auc:0.80817\ttest-logloss:0.52436\n",
      "[250]\ttrain-error:0.07544\ttrain-auc:0.97864\ttrain-logloss:0.31750\ttest-error:0.28125\ttest-auc:0.80770\ttest-logloss:0.52511\n",
      "[251]\ttrain-error:0.07556\ttrain-auc:0.97871\ttrain-logloss:0.31737\ttest-error:0.28100\ttest-auc:0.80768\ttest-logloss:0.52515\n",
      "Stopping after 252 rounds                                                        \n",
      "[0]\ttrain-error:0.26275\ttrain-auc:0.80847\ttrain-logloss:0.68570\ttest-error:0.29575\ttest-auc:0.77245\ttest-logloss:0.68659\n",
      "[50]\ttrain-error:0.22625\ttrain-auc:0.86533\ttrain-logloss:0.51658\ttest-error:0.27025\ttest-auc:0.81128\ttest-logloss:0.55131\n",
      "[100]\ttrain-error:0.21731\ttrain-auc:0.87867\ttrain-logloss:0.46501\ttest-error:0.27175\ttest-auc:0.81119\ttest-logloss:0.52690\n",
      "[150]\ttrain-error:0.20938\ttrain-auc:0.88585\ttrain-logloss:0.44516\ttest-error:0.27325\ttest-auc:0.81164\ttest-logloss:0.52162\n",
      "[200]\ttrain-error:0.20538\ttrain-auc:0.89150\ttrain-logloss:0.43413\ttest-error:0.27325\ttest-auc:0.81181\ttest-logloss:0.52013\n",
      "[246]\ttrain-error:0.20044\ttrain-auc:0.89662\ttrain-logloss:0.42706\ttest-error:0.27225\ttest-auc:0.81135\ttest-logloss:0.52040\n",
      "Stopping after 248 rounds                                                         \n",
      "[0]\ttrain-error:0.24544\ttrain-auc:0.83367\ttrain-logloss:0.66769\ttest-error:0.32975\ttest-auc:0.74120\ttest-logloss:0.67569\n",
      "[50]\ttrain-error:0.08725\ttrain-auc:0.97297\ttrain-logloss:0.32828\ttest-error:0.27875\ttest-auc:0.80141\ttest-logloss:0.53340\n",
      "[90]\ttrain-error:0.05200\ttrain-auc:0.98816\ttrain-logloss:0.27986\ttest-error:0.28425\ttest-auc:0.79986\ttest-logloss:0.53796\n",
      "Stopping after 92 rounds                                                          \n",
      "[0]\ttrain-error:0.27869\ttrain-auc:0.80589\ttrain-logloss:0.63085\ttest-error:0.30575\ttest-auc:0.77811\ttest-logloss:0.63728\n",
      "[50]\ttrain-error:0.18794\ttrain-auc:0.90334\ttrain-logloss:0.40533\ttest-error:0.29625\ttest-auc:0.78770\ttest-logloss:0.55230\n",
      "[69]\ttrain-error:0.16087\ttrain-auc:0.92620\ttrain-logloss:0.37074\ttest-error:0.29900\ttest-auc:0.78689\ttest-logloss:0.55993\n",
      "Stopping after 70 rounds                                                          \n",
      "[0]\ttrain-error:0.26325\ttrain-auc:0.82132\ttrain-logloss:0.68547\ttest-error:0.30025\ttest-auc:0.77568\ttest-logloss:0.68659\n",
      "[50]\ttrain-error:0.23669\ttrain-auc:0.86820\ttrain-logloss:0.50947\ttest-error:0.26875\ttest-auc:0.81296\ttest-logloss:0.54666\n",
      "[100]\ttrain-error:0.22275\ttrain-auc:0.87854\ttrain-logloss:0.46217\ttest-error:0.26875\ttest-auc:0.81240\ttest-logloss:0.52409\n",
      "[150]\ttrain-error:0.21338\ttrain-auc:0.88785\ttrain-logloss:0.44204\ttest-error:0.27100\ttest-auc:0.81292\ttest-logloss:0.51928\n",
      "[200]\ttrain-error:0.20744\ttrain-auc:0.89502\ttrain-logloss:0.43038\ttest-error:0.27225\ttest-auc:0.81261\ttest-logloss:0.51869\n",
      "[250]\ttrain-error:0.19750\ttrain-auc:0.90500\ttrain-logloss:0.41852\ttest-error:0.27275\ttest-auc:0.81172\ttest-logloss:0.51935\n",
      "[269]\ttrain-error:0.19412\ttrain-auc:0.90859\ttrain-logloss:0.41412\ttest-error:0.27050\ttest-auc:0.81185\ttest-logloss:0.51934\n",
      "Stopping after 270 rounds                                                         \n",
      "[0]\ttrain-error:0.27163\ttrain-auc:0.81080\ttrain-logloss:0.67199\ttest-error:0.29500\ttest-auc:0.77497\ttest-logloss:0.67453\n",
      "[50]\ttrain-error:0.17194\ttrain-auc:0.91263\ttrain-logloss:0.41488\ttest-error:0.27250\ttest-auc:0.80847\ttest-logloss:0.52395\n",
      "[100]\ttrain-error:0.13856\ttrain-auc:0.93994\ttrain-logloss:0.37189\ttest-error:0.27650\ttest-auc:0.80698\ttest-logloss:0.52699\n",
      "[103]\ttrain-error:0.13562\ttrain-auc:0.94224\ttrain-logloss:0.36846\ttest-error:0.27425\ttest-auc:0.80711\ttest-logloss:0.52694\n",
      "Stopping after 104 rounds                                                           \n",
      "[0]\ttrain-error:0.27069\ttrain-auc:0.81455\ttrain-logloss:0.67466\ttest-error:0.29025\ttest-auc:0.77781\ttest-logloss:0.67727\n",
      "[50]\ttrain-error:0.17925\ttrain-auc:0.90976\ttrain-logloss:0.42192\ttest-error:0.27825\ttest-auc:0.80683\ttest-logloss:0.52652\n",
      "[100]\ttrain-error:0.14913\ttrain-auc:0.93846\ttrain-logloss:0.37531\ttest-error:0.27800\ttest-auc:0.80528\ttest-logloss:0.52775\n",
      "[106]\ttrain-error:0.14012\ttrain-auc:0.94406\ttrain-logloss:0.36781\ttest-error:0.27925\ttest-auc:0.80435\ttest-logloss:0.52870\n",
      "Stopping after 107 rounds                                                         \n",
      "[0]\ttrain-error:0.28144\ttrain-auc:0.80377\ttrain-logloss:0.68523\ttest-error:0.30700\ttest-auc:0.78125\ttest-logloss:0.68581\n",
      "[50]\ttrain-error:0.24506\ttrain-auc:0.84266\ttrain-logloss:0.52434\ttest-error:0.27600\ttest-auc:0.81096\ttest-logloss:0.54511\n",
      "[100]\ttrain-error:0.23419\ttrain-auc:0.85503\ttrain-logloss:0.48629\ttest-error:0.27425\ttest-auc:0.81126\ttest-logloss:0.52485\n",
      "[150]\ttrain-error:0.22569\ttrain-auc:0.86391\ttrain-logloss:0.46901\ttest-error:0.27425\ttest-auc:0.81130\ttest-logloss:0.52111\n",
      "[200]\ttrain-error:0.22162\ttrain-auc:0.87048\ttrain-logloss:0.45948\ttest-error:0.27275\ttest-auc:0.81143\ttest-logloss:0.51985\n",
      "[250]\ttrain-error:0.21425\ttrain-auc:0.87751\ttrain-logloss:0.45063\ttest-error:0.27575\ttest-auc:0.81142\ttest-logloss:0.51943\n",
      "[297]\ttrain-error:0.20863\ttrain-auc:0.88316\ttrain-logloss:0.44410\ttest-error:0.27450\ttest-auc:0.81098\ttest-logloss:0.51967\n",
      "Stopping after 298 rounds                                                         \n",
      "[0]\ttrain-error:0.17831\ttrain-auc:0.89026\ttrain-logloss:0.58156\ttest-error:0.34400\ttest-auc:0.72671\ttest-logloss:0.64412\n",
      "[50]\ttrain-error:0.00013\ttrain-auc:1.00000\ttrain-logloss:0.06545\ttest-error:0.29275\ttest-auc:0.78637\ttest-logloss:0.62591\n",
      "[60]\ttrain-error:0.00006\ttrain-auc:1.00000\ttrain-logloss:0.05217\ttest-error:0.29575\ttest-auc:0.78871\ttest-logloss:0.63421\n",
      "Stopping after 61 rounds                                                          \n",
      "[0]\ttrain-error:0.24419\ttrain-auc:0.83828\ttrain-logloss:0.66821\ttest-error:0.33675\ttest-auc:0.73468\ttest-logloss:0.67696\n",
      "[50]\ttrain-error:0.07562\ttrain-auc:0.98034\ttrain-logloss:0.31645\ttest-error:0.28775\ttest-auc:0.80002\ttest-logloss:0.53382\n",
      "[100]\ttrain-error:0.03719\ttrain-auc:0.99412\ttrain-logloss:0.25909\ttest-error:0.28500\ttest-auc:0.80121\ttest-logloss:0.53653\n",
      "[110]\ttrain-error:0.02894\ttrain-auc:0.99575\ttrain-logloss:0.24782\ttest-error:0.28875\ttest-auc:0.80047\ttest-logloss:0.53808\n",
      "Stopping after 112 rounds                                                         \n",
      "[0]\ttrain-error:0.23813\ttrain-auc:0.85069\ttrain-logloss:0.62934\ttest-error:0.32200\ttest-auc:0.74104\ttest-logloss:0.65338\n",
      "[50]\ttrain-error:0.01038\ttrain-auc:0.99941\ttrain-logloss:0.17212\ttest-error:0.28825\ttest-auc:0.79466\ttest-logloss:0.56677\n",
      "[62]\ttrain-error:0.00394\ttrain-auc:0.99992\ttrain-logloss:0.13757\ttest-error:0.28750\ttest-auc:0.79435\ttest-logloss:0.57539\n",
      "Stopping after 64 rounds                                                          \n",
      "[0]\ttrain-error:0.27487\ttrain-auc:0.80630\ttrain-logloss:0.68405\ttest-error:0.27200\ttest-auc:0.80767\ttest-logloss:0.68397\n",
      "[50]\ttrain-error:0.27050\ttrain-auc:0.81740\ttrain-logloss:0.52990\ttest-error:0.26900\ttest-auc:0.81413\ttest-logloss:0.53313\n",
      "[100]\ttrain-error:0.26881\ttrain-auc:0.82063\ttrain-logloss:0.51054\ttest-error:0.26875\ttest-auc:0.81411\ttest-logloss:0.51911\n",
      "[150]\ttrain-error:0.26713\ttrain-auc:0.82422\ttrain-logloss:0.50413\ttest-error:0.26850\ttest-auc:0.81466\ttest-logloss:0.51692\n",
      "[200]\ttrain-error:0.26512\ttrain-auc:0.82994\ttrain-logloss:0.49758\ttest-error:0.26800\ttest-auc:0.81475\ttest-logloss:0.51658\n",
      "[250]\ttrain-error:0.26269\ttrain-auc:0.83570\ttrain-logloss:0.49080\ttest-error:0.26850\ttest-auc:0.81498\ttest-logloss:0.51624\n",
      "[300]\ttrain-error:0.25819\ttrain-auc:0.84101\ttrain-logloss:0.48477\ttest-error:0.26825\ttest-auc:0.81467\ttest-logloss:0.51642\n",
      "[318]\ttrain-error:0.25750\ttrain-auc:0.84253\ttrain-logloss:0.48288\ttest-error:0.26875\ttest-auc:0.81451\ttest-logloss:0.51668\n",
      "Stopping after 319 rounds                                                         \n",
      "[0]\ttrain-error:0.27175\ttrain-auc:0.80970\ttrain-logloss:0.68440\ttest-error:0.27025\ttest-auc:0.80773\ttest-logloss:0.68447\n",
      "[50]\ttrain-error:0.26538\ttrain-auc:0.82634\ttrain-logloss:0.52490\ttest-error:0.26950\ttest-auc:0.81407\ttest-logloss:0.53502\n",
      "[100]\ttrain-error:0.26081\ttrain-auc:0.83135\ttrain-logloss:0.50054\ttest-error:0.26775\ttest-auc:0.81419\ttest-logloss:0.51925\n",
      "[150]\ttrain-error:0.25844\ttrain-auc:0.83659\ttrain-logloss:0.49153\ttest-error:0.26650\ttest-auc:0.81428\ttest-logloss:0.51731\n",
      "[200]\ttrain-error:0.25675\ttrain-auc:0.84084\ttrain-logloss:0.48518\ttest-error:0.26550\ttest-auc:0.81432\ttest-logloss:0.51701\n",
      "[237]\ttrain-error:0.25562\ttrain-auc:0.84419\ttrain-logloss:0.48081\ttest-error:0.26600\ttest-auc:0.81429\ttest-logloss:0.51712\n",
      "Stopping after 238 rounds                                                         \n",
      "[0]\ttrain-error:0.27506\ttrain-auc:0.80757\ttrain-logloss:0.68318\ttest-error:0.27150\ttest-auc:0.80901\ttest-logloss:0.68308\n",
      "[50]\ttrain-error:0.27037\ttrain-auc:0.81710\ttrain-logloss:0.52695\ttest-error:0.26800\ttest-auc:0.81414\ttest-logloss:0.53008\n",
      "[100]\ttrain-error:0.26844\ttrain-auc:0.82119\ttrain-logloss:0.50936\ttest-error:0.26700\ttest-auc:0.81397\ttest-logloss:0.51855\n",
      "[150]\ttrain-error:0.26562\ttrain-auc:0.82515\ttrain-logloss:0.50304\ttest-error:0.27025\ttest-auc:0.81411\ttest-logloss:0.51718\n",
      "[200]\ttrain-error:0.26400\ttrain-auc:0.83110\ttrain-logloss:0.49607\ttest-error:0.27050\ttest-auc:0.81385\ttest-logloss:0.51696\n",
      "[250]\ttrain-error:0.26162\ttrain-auc:0.83614\ttrain-logloss:0.48981\ttest-error:0.27025\ttest-auc:0.81362\ttest-logloss:0.51712\n",
      "[276]\ttrain-error:0.26131\ttrain-auc:0.83822\ttrain-logloss:0.48714\ttest-error:0.27050\ttest-auc:0.81345\ttest-logloss:0.51748\n",
      "Stopping after 277 rounds                                                         \n",
      "[0]\ttrain-error:0.27513\ttrain-auc:0.80747\ttrain-logloss:0.68338\ttest-error:0.27100\ttest-auc:0.80861\ttest-logloss:0.68332\n",
      "[50]\ttrain-error:0.27300\ttrain-auc:0.81686\ttrain-logloss:0.52787\ttest-error:0.27000\ttest-auc:0.81369\ttest-logloss:0.53103\n",
      "[100]\ttrain-error:0.26919\ttrain-auc:0.82060\ttrain-logloss:0.51017\ttest-error:0.26850\ttest-auc:0.81411\ttest-logloss:0.51846\n",
      "[150]\ttrain-error:0.26737\ttrain-auc:0.82417\ttrain-logloss:0.50408\ttest-error:0.26725\ttest-auc:0.81410\ttest-logloss:0.51719\n",
      "[200]\ttrain-error:0.26506\ttrain-auc:0.82909\ttrain-logloss:0.49805\ttest-error:0.26925\ttest-auc:0.81403\ttest-logloss:0.51703\n",
      "[250]\ttrain-error:0.26225\ttrain-auc:0.83481\ttrain-logloss:0.49144\ttest-error:0.26875\ttest-auc:0.81409\ttest-logloss:0.51693\n",
      "[265]\ttrain-error:0.25981\ttrain-auc:0.83649\ttrain-logloss:0.48965\ttest-error:0.26875\ttest-auc:0.81422\ttest-logloss:0.51686\n",
      "Stopping after 266 rounds                                                         \n",
      "[0]\ttrain-error:0.27450\ttrain-auc:0.80686\ttrain-logloss:0.68263\ttest-error:0.26900\ttest-auc:0.81027\ttest-logloss:0.68249\n",
      "[50]\ttrain-error:0.27063\ttrain-auc:0.81783\ttrain-logloss:0.52449\ttest-error:0.26950\ttest-auc:0.81384\ttest-logloss:0.52873\n",
      "[100]\ttrain-error:0.26819\ttrain-auc:0.82184\ttrain-logloss:0.50812\ttest-error:0.26850\ttest-auc:0.81397\ttest-logloss:0.51869\n",
      "[150]\ttrain-error:0.26562\ttrain-auc:0.82639\ttrain-logloss:0.50127\ttest-error:0.26725\ttest-auc:0.81418\ttest-logloss:0.51763\n",
      "[200]\ttrain-error:0.26231\ttrain-auc:0.83300\ttrain-logloss:0.49389\ttest-error:0.26950\ttest-auc:0.81435\ttest-logloss:0.51700\n",
      "[250]\ttrain-error:0.25994\ttrain-auc:0.83903\ttrain-logloss:0.48647\ttest-error:0.26975\ttest-auc:0.81405\ttest-logloss:0.51703\n",
      "[271]\ttrain-error:0.25788\ttrain-auc:0.84202\ttrain-logloss:0.48345\ttest-error:0.27050\ttest-auc:0.81409\ttest-logloss:0.51707\n",
      "Stopping after 273 rounds                                                         \n",
      "[0]\ttrain-error:0.27300\ttrain-auc:0.81296\ttrain-logloss:0.68092\ttest-error:0.28125\ttest-auc:0.80076\ttest-logloss:0.68136\n",
      "[50]\ttrain-error:0.25881\ttrain-auc:0.84122\ttrain-logloss:0.49915\ttest-error:0.27050\ttest-auc:0.81319\ttest-logloss:0.52607\n",
      "[100]\ttrain-error:0.25056\ttrain-auc:0.85165\ttrain-logloss:0.47601\ttest-error:0.27050\ttest-auc:0.81226\ttest-logloss:0.51950\n",
      "[150]\ttrain-error:0.24325\ttrain-auc:0.86053\ttrain-logloss:0.46410\ttest-error:0.26825\ttest-auc:0.81266\ttest-logloss:0.51869\n",
      "[197]\ttrain-error:0.23162\ttrain-auc:0.87446\ttrain-logloss:0.44960\ttest-error:0.27200\ttest-auc:0.81215\ttest-logloss:0.51935\n",
      "Stopping after 199 rounds                                                           \n",
      "[0]\ttrain-error:0.26256\ttrain-auc:0.82691\ttrain-logloss:0.66606\ttest-error:0.29850\ttest-auc:0.78520\ttest-logloss:0.66960\n",
      "[50]\ttrain-error:0.17700\ttrain-auc:0.91375\ttrain-logloss:0.41056\ttest-error:0.27275\ttest-auc:0.81027\ttest-logloss:0.52190\n",
      "[97]\ttrain-error:0.14775\ttrain-auc:0.93729\ttrain-logloss:0.37865\ttest-error:0.27525\ttest-auc:0.80824\ttest-logloss:0.52464\n",
      "Stopping after 98 rounds                                                            \n",
      "[0]\ttrain-error:0.27500\ttrain-auc:0.81019\ttrain-logloss:0.68484\ttest-error:0.27225\ttest-auc:0.80356\ttest-logloss:0.68496\n",
      "[50]\ttrain-error:0.26594\ttrain-auc:0.82715\ttrain-logloss:0.52674\ttest-error:0.26950\ttest-auc:0.81471\ttest-logloss:0.53703\n",
      "[100]\ttrain-error:0.26219\ttrain-auc:0.83353\ttrain-logloss:0.49838\ttest-error:0.26575\ttest-auc:0.81438\ttest-logloss:0.51957\n",
      "[150]\ttrain-error:0.25800\ttrain-auc:0.83969\ttrain-logloss:0.48707\ttest-error:0.26800\ttest-auc:0.81436\ttest-logloss:0.51706\n",
      "[200]\ttrain-error:0.25425\ttrain-auc:0.84658\ttrain-logloss:0.47756\ttest-error:0.26900\ttest-auc:0.81454\ttest-logloss:0.51657\n",
      "[239]\ttrain-error:0.25106\ttrain-auc:0.85211\ttrain-logloss:0.47098\ttest-error:0.26825\ttest-auc:0.81429\ttest-logloss:0.51649\n",
      "Stopping after 241 rounds                                                         \n",
      "[0]\ttrain-error:0.27613\ttrain-auc:0.80871\ttrain-logloss:0.66544\ttest-error:0.27475\ttest-auc:0.80424\ttest-logloss:0.66606\n",
      "[50]\ttrain-error:0.25325\ttrain-auc:0.84366\ttrain-logloss:0.48193\ttest-error:0.27500\ttest-auc:0.81145\ttest-logloss:0.52007\n",
      "[100]\ttrain-error:0.23025\ttrain-auc:0.87281\ttrain-logloss:0.44813\ttest-error:0.27250\ttest-auc:0.80887\ttest-logloss:0.52345\n",
      "[106]\ttrain-error:0.22719\ttrain-auc:0.87680\ttrain-logloss:0.44376\ttest-error:0.27575\ttest-auc:0.80892\ttest-logloss:0.52386\n",
      "Stopping after 107 rounds                                                         \n",
      "[0]\ttrain-error:0.26737\ttrain-auc:0.81884\ttrain-logloss:0.68451\ttest-error:0.27300\ttest-auc:0.79711\ttest-logloss:0.68508\n",
      "[50]\ttrain-error:0.24000\ttrain-auc:0.86222\ttrain-logloss:0.50142\ttest-error:0.27050\ttest-auc:0.81354\ttest-logloss:0.53773\n",
      "[100]\ttrain-error:0.23075\ttrain-auc:0.87314\ttrain-logloss:0.46029\ttest-error:0.26725\ttest-auc:0.81388\ttest-logloss:0.51976\n",
      "[150]\ttrain-error:0.22087\ttrain-auc:0.88224\ttrain-logloss:0.44358\ttest-error:0.26750\ttest-auc:0.81389\ttest-logloss:0.51710\n",
      "[200]\ttrain-error:0.21363\ttrain-auc:0.89133\ttrain-logloss:0.43137\ttest-error:0.26775\ttest-auc:0.81329\ttest-logloss:0.51757\n",
      "[209]\ttrain-error:0.21181\ttrain-auc:0.89347\ttrain-logloss:0.42876\ttest-error:0.26850\ttest-auc:0.81318\ttest-logloss:0.51772\n",
      "Stopping after 210 rounds                                                         \n",
      "[0]\ttrain-error:0.23500\ttrain-auc:0.85514\ttrain-logloss:0.67535\ttest-error:0.30225\ttest-auc:0.77291\ttest-logloss:0.67966\n",
      "[50]\ttrain-error:0.15531\ttrain-auc:0.93723\ttrain-logloss:0.40111\ttest-error:0.27500\ttest-auc:0.81034\ttest-logloss:0.52440\n",
      "[100]\ttrain-error:0.12606\ttrain-auc:0.95692\ttrain-logloss:0.35918\ttest-error:0.26900\ttest-auc:0.81050\ttest-logloss:0.52109\n",
      "[146]\ttrain-error:0.10950\ttrain-auc:0.96699\ttrain-logloss:0.34387\ttest-error:0.27475\ttest-auc:0.80966\ttest-logloss:0.52294\n",
      "Stopping after 148 rounds                                                         \n",
      "[0]\ttrain-error:0.27350\ttrain-auc:0.80915\ttrain-logloss:0.68064\ttest-error:0.28400\ttest-auc:0.80233\ttest-logloss:0.68094\n",
      "[50]\ttrain-error:0.25994\ttrain-auc:0.83111\ttrain-logloss:0.50668\ttest-error:0.26750\ttest-auc:0.81407\ttest-logloss:0.52409\n",
      "[100]\ttrain-error:0.25669\ttrain-auc:0.84067\ttrain-logloss:0.48506\ttest-error:0.26650\ttest-auc:0.81425\ttest-logloss:0.51757\n",
      "[150]\ttrain-error:0.25025\ttrain-auc:0.85343\ttrain-logloss:0.46927\ttest-error:0.26425\ttest-auc:0.81469\ttest-logloss:0.51727\n",
      "[199]\ttrain-error:0.23981\ttrain-auc:0.86757\ttrain-logloss:0.45334\ttest-error:0.26700\ttest-auc:0.81371\ttest-logloss:0.51805\n",
      "Stopping after 200 rounds                                                         \n",
      "[0]\ttrain-error:0.26450\ttrain-auc:0.82101\ttrain-logloss:0.65981\ttest-error:0.28200\ttest-auc:0.79226\ttest-logloss:0.66314\n",
      "[50]\ttrain-error:0.22575\ttrain-auc:0.87782\ttrain-logloss:0.44403\ttest-error:0.26850\ttest-auc:0.81202\ttest-logloss:0.51893\n",
      "[100]\ttrain-error:0.17525\ttrain-auc:0.92168\ttrain-logloss:0.39505\ttest-error:0.28100\ttest-auc:0.80763\ttest-logloss:0.52477\n",
      "[101]\ttrain-error:0.17444\ttrain-auc:0.92269\ttrain-logloss:0.39393\ttest-error:0.28075\ttest-auc:0.80770\ttest-logloss:0.52470\n",
      "Stopping after 103 rounds                                                         \n",
      "[0]\ttrain-error:0.26963\ttrain-auc:0.81206\ttrain-logloss:0.64943\ttest-error:0.28450\ttest-auc:0.78881\ttest-logloss:0.65295\n",
      "[50]\ttrain-error:0.20694\ttrain-auc:0.89472\ttrain-logloss:0.42236\ttest-error:0.27550\ttest-auc:0.80907\ttest-logloss:0.52398\n",
      "[90]\ttrain-error:0.14025\ttrain-auc:0.94265\ttrain-logloss:0.36154\ttest-error:0.27975\ttest-auc:0.80448\ttest-logloss:0.53220\n",
      "Stopping after 91 rounds                                                          \n",
      "[0]\ttrain-error:0.21900\ttrain-auc:0.86004\ttrain-logloss:0.68097\ttest-error:0.31250\ttest-auc:0.76001\ttest-logloss:0.68455\n",
      "[50]\ttrain-error:0.13544\ttrain-auc:0.94833\ttrain-logloss:0.42361\ttest-error:0.27125\ttest-auc:0.81065\ttest-logloss:0.53705\n",
      "[100]\ttrain-error:0.10325\ttrain-auc:0.96954\ttrain-logloss:0.35057\ttest-error:0.26975\ttest-auc:0.81128\ttest-logloss:0.52187\n",
      "[150]\ttrain-error:0.07919\ttrain-auc:0.98081\ttrain-logloss:0.31969\ttest-error:0.26800\ttest-auc:0.81065\ttest-logloss:0.52194\n",
      "[162]\ttrain-error:0.07594\ttrain-auc:0.98242\ttrain-logloss:0.31611\ttest-error:0.26875\ttest-auc:0.81072\ttest-logloss:0.52211\n",
      "Stopping after 164 rounds                                                         \n",
      "[0]\ttrain-error:0.18531\ttrain-auc:0.91036\ttrain-logloss:0.66634\ttest-error:0.32800\ttest-auc:0.75498\ttest-logloss:0.67756\n",
      "[50]\ttrain-error:0.00581\ttrain-auc:0.99897\ttrain-logloss:0.22246\ttest-error:0.27900\ttest-auc:0.80437\ttest-logloss:0.52974\n",
      "[95]\ttrain-error:0.00150\ttrain-auc:0.99962\ttrain-logloss:0.17006\ttest-error:0.27775\ttest-auc:0.80551\ttest-logloss:0.53443\n",
      "Stopping after 96 rounds                                                          \n",
      "[0]\ttrain-error:0.27106\ttrain-auc:0.81718\ttrain-logloss:0.68535\ttest-error:0.29475\ttest-auc:0.79428\ttest-logloss:0.68588\n",
      "[50]\ttrain-error:0.25488\ttrain-auc:0.84131\ttrain-logloss:0.52177\ttest-error:0.26575\ttest-auc:0.81387\ttest-logloss:0.54193\n",
      "[100]\ttrain-error:0.24738\ttrain-auc:0.85032\ttrain-logloss:0.48466\ttest-error:0.26525\ttest-auc:0.81454\ttest-logloss:0.52086\n",
      "[150]\ttrain-error:0.24150\ttrain-auc:0.85722\ttrain-logloss:0.47026\ttest-error:0.26625\ttest-auc:0.81454\ttest-logloss:0.51741\n",
      "[200]\ttrain-error:0.23825\ttrain-auc:0.86360\ttrain-logloss:0.46090\ttest-error:0.26675\ttest-auc:0.81426\ttest-logloss:0.51713\n",
      "[250]\ttrain-error:0.23281\ttrain-auc:0.87138\ttrain-logloss:0.45142\ttest-error:0.26475\ttest-auc:0.81410\ttest-logloss:0.51713\n",
      "[264]\ttrain-error:0.23106\ttrain-auc:0.87338\ttrain-logloss:0.44903\ttest-error:0.26550\ttest-auc:0.81409\ttest-logloss:0.51719\n",
      "Stopping after 266 rounds                                                         \n",
      "[0]\ttrain-error:0.25238\ttrain-auc:0.83158\ttrain-logloss:0.66791\ttest-error:0.29600\ttest-auc:0.78026\ttest-logloss:0.67185\n",
      "[50]\ttrain-error:0.14150\ttrain-auc:0.93920\ttrain-logloss:0.38073\ttest-error:0.28250\ttest-auc:0.80891\ttest-logloss:0.52349\n",
      "[100]\ttrain-error:0.09438\ttrain-auc:0.96757\ttrain-logloss:0.33155\ttest-error:0.27525\ttest-auc:0.80720\ttest-logloss:0.52745\n",
      "[102]\ttrain-error:0.09194\ttrain-auc:0.96881\ttrain-logloss:0.32908\ttest-error:0.27550\ttest-auc:0.80738\ttest-logloss:0.52726\n",
      "Stopping after 103 rounds                                                         \n",
      "[0]\ttrain-error:0.27394\ttrain-auc:0.81061\ttrain-logloss:0.68165\ttest-error:0.26925\ttest-auc:0.80807\ttest-logloss:0.68179\n",
      "[50]\ttrain-error:0.26481\ttrain-auc:0.83240\ttrain-logloss:0.50844\ttest-error:0.26800\ttest-auc:0.81487\ttest-logloss:0.52567\n",
      "[100]\ttrain-error:0.25731\ttrain-auc:0.84183\ttrain-logloss:0.48488\ttest-error:0.26700\ttest-auc:0.81461\ttest-logloss:0.51736\n",
      "[150]\ttrain-error:0.25175\ttrain-auc:0.85125\ttrain-logloss:0.47144\ttest-error:0.26700\ttest-auc:0.81446\ttest-logloss:0.51731\n",
      "[166]\ttrain-error:0.24888\ttrain-auc:0.85545\ttrain-logloss:0.46645\ttest-error:0.26750\ttest-auc:0.81443\ttest-logloss:0.51750\n",
      "Stopping after 168 rounds                                                         \n",
      "[0]\ttrain-error:0.23281\ttrain-auc:0.85293\ttrain-logloss:0.68134\ttest-error:0.29425\ttest-auc:0.77308\ttest-logloss:0.68412\n",
      "[50]\ttrain-error:0.15769\ttrain-auc:0.93870\ttrain-logloss:0.42599\ttest-error:0.27350\ttest-auc:0.80904\ttest-logloss:0.53489\n",
      "[100]\ttrain-error:0.11700\ttrain-auc:0.96319\ttrain-logloss:0.35636\ttest-error:0.27525\ttest-auc:0.81007\ttest-logloss:0.52277\n",
      "[150]\ttrain-error:0.08825\ttrain-auc:0.97817\ttrain-logloss:0.32503\ttest-error:0.27300\ttest-auc:0.81012\ttest-logloss:0.52289\n",
      "[167]\ttrain-error:0.07787\ttrain-auc:0.98279\ttrain-logloss:0.31623\ttest-error:0.27125\ttest-auc:0.81011\ttest-logloss:0.52326\n",
      "Stopping after 169 rounds                                                         \n",
      "[0]\ttrain-error:0.26825\ttrain-auc:0.81174\ttrain-logloss:0.68719\ttest-error:0.27625\ttest-auc:0.79408\ttest-logloss:0.68753\n",
      "[50]\ttrain-error:0.25412\ttrain-auc:0.84108\ttrain-logloss:0.54063\ttest-error:0.27175\ttest-auc:0.81337\ttest-logloss:0.55732\n",
      "[100]\ttrain-error:0.25038\ttrain-auc:0.84935\ttrain-logloss:0.49571\ttest-error:0.27025\ttest-auc:0.81313\ttest-logloss:0.52817\n",
      "[150]\ttrain-error:0.24569\ttrain-auc:0.85646\ttrain-logloss:0.47527\ttest-error:0.27050\ttest-auc:0.81304\ttest-logloss:0.52100\n",
      "[200]\ttrain-error:0.24231\ttrain-auc:0.86250\ttrain-logloss:0.46294\ttest-error:0.26975\ttest-auc:0.81301\ttest-logloss:0.51906\n",
      "[250]\ttrain-error:0.23431\ttrain-auc:0.87246\ttrain-logloss:0.45059\ttest-error:0.27125\ttest-auc:0.81258\ttest-logloss:0.51907\n",
      "[300]\ttrain-error:0.22800\ttrain-auc:0.88120\ttrain-logloss:0.43963\ttest-error:0.26950\ttest-auc:0.81260\ttest-logloss:0.51893\n",
      "[350]\ttrain-error:0.21388\ttrain-auc:0.89372\ttrain-logloss:0.42607\ttest-error:0.26925\ttest-auc:0.81309\ttest-logloss:0.51845\n",
      "[387]\ttrain-error:0.20425\ttrain-auc:0.90318\ttrain-logloss:0.41611\ttest-error:0.26950\ttest-auc:0.81277\ttest-logloss:0.51877\n",
      "Stopping after 389 rounds                                                         \n",
      "[0]\ttrain-error:0.25600\ttrain-auc:0.83226\ttrain-logloss:0.60145\ttest-error:0.33475\ttest-auc:0.75203\ttest-logloss:0.63081\n",
      "[50]\ttrain-error:0.02156\ttrain-auc:0.99738\ttrain-logloss:0.16709\ttest-error:0.30900\ttest-auc:0.77152\ttest-logloss:0.63770\n",
      "[56]\ttrain-error:0.01344\ttrain-auc:0.99849\ttrain-logloss:0.14742\ttest-error:0.30650\ttest-auc:0.76928\ttest-logloss:0.65153\n",
      "Stopping after 57 rounds                                                          \n",
      "[0]\ttrain-error:0.25281\ttrain-auc:0.83395\ttrain-logloss:0.67660\ttest-error:0.30550\ttest-auc:0.76229\ttest-logloss:0.68047\n",
      "[50]\ttrain-error:0.14838\ttrain-auc:0.93773\ttrain-logloss:0.40134\ttest-error:0.28175\ttest-auc:0.80736\ttest-logloss:0.52749\n",
      "[100]\ttrain-error:0.11656\ttrain-auc:0.95757\ttrain-logloss:0.35489\ttest-error:0.28300\ttest-auc:0.80598\ttest-logloss:0.52544\n",
      "[133]\ttrain-error:0.10400\ttrain-auc:0.96572\ttrain-logloss:0.34028\ttest-error:0.28025\ttest-auc:0.80651\ttest-logloss:0.52588\n",
      "Stopping after 135 rounds                                                         \n",
      "[0]\ttrain-error:0.24806\ttrain-auc:0.83962\ttrain-logloss:0.68437\ttest-error:0.29200\ttest-auc:0.77457\ttest-logloss:0.68605\n",
      "[50]\ttrain-error:0.20212\ttrain-auc:0.90490\ttrain-logloss:0.47509\ttest-error:0.26850\ttest-auc:0.81272\ttest-logloss:0.54283\n",
      "[100]\ttrain-error:0.17469\ttrain-auc:0.92753\ttrain-logloss:0.40788\ttest-error:0.26950\ttest-auc:0.81270\ttest-logloss:0.52210\n",
      "[150]\ttrain-error:0.15119\ttrain-auc:0.94421\ttrain-logloss:0.37545\ttest-error:0.27025\ttest-auc:0.81263\ttest-logloss:0.51950\n",
      "[200]\ttrain-error:0.13037\ttrain-auc:0.95909\ttrain-logloss:0.35243\ttest-error:0.27375\ttest-auc:0.81222\ttest-logloss:0.52046\n",
      "[201]\ttrain-error:0.12962\ttrain-auc:0.95962\ttrain-logloss:0.35165\ttest-error:0.27375\ttest-auc:0.81206\ttest-logloss:0.52061\n",
      "Stopping after 202 rounds                                                         \n",
      "[0]\ttrain-error:0.27531\ttrain-auc:0.80691\ttrain-logloss:0.65765\ttest-error:0.27050\ttest-auc:0.80731\ttest-logloss:0.65747\n",
      "[50]\ttrain-error:0.26488\ttrain-auc:0.83203\ttrain-logloss:0.49290\ttest-error:0.27275\ttest-auc:0.81203\ttest-logloss:0.51928\n",
      "[91]\ttrain-error:0.25625\ttrain-auc:0.84906\ttrain-logloss:0.47006\ttest-error:0.27150\ttest-auc:0.81231\ttest-logloss:0.51914\n",
      "Stopping after 93 rounds                                                          \n",
      "[0]\ttrain-error:0.25938\ttrain-auc:0.82153\ttrain-logloss:0.68714\ttest-error:0.28650\ttest-auc:0.78243\ttest-logloss:0.68785\n",
      "[50]\ttrain-error:0.24000\ttrain-auc:0.86139\ttrain-logloss:0.53092\ttest-error:0.26975\ttest-auc:0.81354\ttest-logloss:0.55946\n",
      "[100]\ttrain-error:0.23237\ttrain-auc:0.87196\ttrain-logloss:0.47772\ttest-error:0.27000\ttest-auc:0.81359\ttest-logloss:0.52892\n",
      "[150]\ttrain-error:0.22419\ttrain-auc:0.88231\ttrain-logloss:0.45049\ttest-error:0.27000\ttest-auc:0.81318\ttest-logloss:0.52099\n",
      "[200]\ttrain-error:0.21756\ttrain-auc:0.89114\ttrain-logloss:0.43426\ttest-error:0.26900\ttest-auc:0.81298\ttest-logloss:0.51890\n",
      "[250]\ttrain-error:0.20756\ttrain-auc:0.90194\ttrain-logloss:0.41970\ttest-error:0.27075\ttest-auc:0.81288\ttest-logloss:0.51847\n",
      "[300]\ttrain-error:0.19637\ttrain-auc:0.91277\ttrain-logloss:0.40634\ttest-error:0.26975\ttest-auc:0.81292\ttest-logloss:0.51831\n",
      "[319]\ttrain-error:0.19100\ttrain-auc:0.91781\ttrain-logloss:0.40050\ttest-error:0.27100\ttest-auc:0.81287\ttest-logloss:0.51844\n",
      "Stopping after 320 rounds                                                         \n",
      "[0]\ttrain-error:0.26750\ttrain-auc:0.81829\ttrain-logloss:0.68629\ttest-error:0.29000\ttest-auc:0.78405\ttest-logloss:0.68702\n",
      "[50]\ttrain-error:0.22775\ttrain-auc:0.87385\ttrain-logloss:0.51384\ttest-error:0.27200\ttest-auc:0.81096\ttest-logloss:0.55289\n",
      "[100]\ttrain-error:0.21650\ttrain-auc:0.88477\ttrain-logloss:0.46211\ttest-error:0.27125\ttest-auc:0.81157\ttest-logloss:0.52650\n",
      "[150]\ttrain-error:0.20613\ttrain-auc:0.89407\ttrain-logloss:0.43865\ttest-error:0.27300\ttest-auc:0.81123\ttest-logloss:0.52116\n",
      "[200]\ttrain-error:0.19937\ttrain-auc:0.90028\ttrain-logloss:0.42764\ttest-error:0.27425\ttest-auc:0.81134\ttest-logloss:0.52006\n",
      "[250]\ttrain-error:0.18794\ttrain-auc:0.91021\ttrain-logloss:0.41505\ttest-error:0.27650\ttest-auc:0.81054\ttest-logloss:0.52057\n",
      "[263]\ttrain-error:0.18281\ttrain-auc:0.91362\ttrain-logloss:0.41109\ttest-error:0.27575\ttest-auc:0.81044\ttest-logloss:0.52059\n",
      "Stopping after 264 rounds                                                         \n",
      "[0]\ttrain-error:0.24288\ttrain-auc:0.84871\ttrain-logloss:0.66841\ttest-error:0.31000\ttest-auc:0.77141\ttest-logloss:0.67403\n",
      "[50]\ttrain-error:0.15006\ttrain-auc:0.93771\ttrain-logloss:0.38566\ttest-error:0.26925\ttest-auc:0.81260\ttest-logloss:0.51893\n",
      "[100]\ttrain-error:0.12131\ttrain-auc:0.95939\ttrain-logloss:0.35099\ttest-error:0.27150\ttest-auc:0.81115\ttest-logloss:0.52086\n",
      "[109]\ttrain-error:0.11488\ttrain-auc:0.96336\ttrain-logloss:0.34544\ttest-error:0.27225\ttest-auc:0.81097\ttest-logloss:0.52121\n",
      "Stopping after 111 rounds                                                         \n",
      "[0]\ttrain-error:0.26325\ttrain-auc:0.82078\ttrain-logloss:0.68428\ttest-error:0.29725\ttest-auc:0.78131\ttest-logloss:0.68537\n",
      "[50]\ttrain-error:0.21275\ttrain-auc:0.88184\ttrain-logloss:0.48969\ttest-error:0.26825\ttest-auc:0.81162\ttest-logloss:0.54006\n",
      "[100]\ttrain-error:0.18950\ttrain-auc:0.90001\ttrain-logloss:0.43814\ttest-error:0.27050\ttest-auc:0.81170\ttest-logloss:0.52190\n",
      "[150]\ttrain-error:0.17563\ttrain-auc:0.91095\ttrain-logloss:0.41729\ttest-error:0.27050\ttest-auc:0.81139\ttest-logloss:0.52017\n",
      "[176]\ttrain-error:0.16819\ttrain-auc:0.91709\ttrain-logloss:0.40892\ttest-error:0.27400\ttest-auc:0.81126\ttest-logloss:0.52007\n",
      "Stopping after 177 rounds                                                         \n",
      "[0]\ttrain-error:0.17969\ttrain-auc:0.90362\ttrain-logloss:0.68096\ttest-error:0.34025\ttest-auc:0.73207\ttest-logloss:0.68674\n",
      "[50]\ttrain-error:0.01713\ttrain-auc:0.99673\ttrain-logloss:0.35333\ttest-error:0.28525\ttest-auc:0.80444\ttest-logloss:0.54972\n",
      "[100]\ttrain-error:0.00625\ttrain-auc:0.99860\ttrain-logloss:0.24029\ttest-error:0.28425\ttest-auc:0.80717\ttest-logloss:0.52730\n",
      "[150]\ttrain-error:0.00306\ttrain-auc:0.99918\ttrain-logloss:0.19455\ttest-error:0.28425\ttest-auc:0.80789\ttest-logloss:0.52663\n",
      "[177]\ttrain-error:0.00219\ttrain-auc:0.99936\ttrain-logloss:0.18406\ttest-error:0.28500\ttest-auc:0.80784\ttest-logloss:0.52761\n",
      "Stopping after 179 rounds                                                         \n",
      "[0]\ttrain-error:0.27019\ttrain-auc:0.81386\ttrain-logloss:0.66762\ttest-error:0.27350\ttest-auc:0.79964\ttest-logloss:0.66860\n",
      "[50]\ttrain-error:0.24744\ttrain-auc:0.85288\ttrain-logloss:0.47423\ttest-error:0.27100\ttest-auc:0.81358\ttest-logloss:0.51846\n",
      "[100]\ttrain-error:0.22200\ttrain-auc:0.87751\ttrain-logloss:0.44537\ttest-error:0.27125\ttest-auc:0.81225\ttest-logloss:0.51977\n",
      "[122]\ttrain-error:0.21638\ttrain-auc:0.88644\ttrain-logloss:0.43473\ttest-error:0.27125\ttest-auc:0.81204\ttest-logloss:0.52027\n",
      "Stopping after 124 rounds                                                         \n",
      "[0]\ttrain-error:0.27650\ttrain-auc:0.80840\ttrain-logloss:0.67659\ttest-error:0.27675\ttest-auc:0.80459\ttest-logloss:0.67692\n",
      "[50]\ttrain-error:0.25844\ttrain-auc:0.83356\ttrain-logloss:0.49773\ttest-error:0.27175\ttest-auc:0.81379\ttest-logloss:0.51989\n",
      "[100]\ttrain-error:0.25250\ttrain-auc:0.84633\ttrain-logloss:0.47743\ttest-error:0.26975\ttest-auc:0.81366\ttest-logloss:0.51732\n",
      "[150]\ttrain-error:0.24319\ttrain-auc:0.86045\ttrain-logloss:0.46141\ttest-error:0.26425\ttest-auc:0.81428\ttest-logloss:0.51720\n",
      "[166]\ttrain-error:0.23938\ttrain-auc:0.86482\ttrain-logloss:0.45642\ttest-error:0.26775\ttest-auc:0.81414\ttest-logloss:0.51765\n",
      "Stopping after 168 rounds                                                         \n",
      "[0]\ttrain-error:0.23975\ttrain-auc:0.84412\ttrain-logloss:0.67687\ttest-error:0.29475\ttest-auc:0.78428\ttest-logloss:0.67977\n",
      "[50]\ttrain-error:0.18294\ttrain-auc:0.91775\ttrain-logloss:0.41985\ttest-error:0.26575\ttest-auc:0.81299\ttest-logloss:0.52312\n",
      "[100]\ttrain-error:0.14994\ttrain-auc:0.94553\ttrain-logloss:0.36914\ttest-error:0.27125\ttest-auc:0.81336\ttest-logloss:0.51929\n",
      "[143]\ttrain-error:0.10631\ttrain-auc:0.97136\ttrain-logloss:0.33145\ttest-error:0.27125\ttest-auc:0.81292\ttest-logloss:0.52098\n",
      "Stopping after 144 rounds                                                         \n",
      "[0]\ttrain-error:0.22981\ttrain-auc:0.85489\ttrain-logloss:0.67877\ttest-error:0.29875\ttest-auc:0.77680\ttest-logloss:0.68201\n",
      "[50]\ttrain-error:0.11850\ttrain-auc:0.95766\ttrain-logloss:0.39053\ttest-error:0.27700\ttest-auc:0.80942\ttest-logloss:0.52903\n",
      "[100]\ttrain-error:0.09087\ttrain-auc:0.97315\ttrain-logloss:0.33486\ttest-error:0.27250\ttest-auc:0.81092\ttest-logloss:0.52152\n",
      "[150]\ttrain-error:0.07656\ttrain-auc:0.98011\ttrain-logloss:0.31777\ttest-error:0.27275\ttest-auc:0.81090\ttest-logloss:0.52254\n",
      "[155]\ttrain-error:0.07575\ttrain-auc:0.98064\ttrain-logloss:0.31677\ttest-error:0.27275\ttest-auc:0.81098\ttest-logloss:0.52265\n",
      "Stopping after 157 rounds                                                         \n",
      "[0]\ttrain-error:0.19162\ttrain-auc:0.88923\ttrain-logloss:0.67636\ttest-error:0.33950\ttest-auc:0.72533\ttest-logloss:0.68427\n",
      "[50]\ttrain-error:0.01044\ttrain-auc:0.99912\ttrain-logloss:0.29831\ttest-error:0.27750\ttest-auc:0.80567\ttest-logloss:0.53518\n",
      "[100]\ttrain-error:0.00137\ttrain-auc:0.99982\ttrain-logloss:0.20456\ttest-error:0.28000\ttest-auc:0.80700\ttest-logloss:0.52793\n",
      "[136]\ttrain-error:0.00069\ttrain-auc:0.99992\ttrain-logloss:0.18101\ttest-error:0.27725\ttest-auc:0.80738\ttest-logloss:0.53032\n",
      "Stopping after 138 rounds                                                         \n",
      "[0]\ttrain-error:0.27581\ttrain-auc:0.80470\ttrain-logloss:0.67844\ttest-error:0.27150\ttest-auc:0.80558\ttest-logloss:0.67843\n",
      "[50]\ttrain-error:0.26887\ttrain-auc:0.81996\ttrain-logloss:0.51455\ttest-error:0.27025\ttest-auc:0.81361\ttest-logloss:0.52185\n",
      "[100]\ttrain-error:0.26700\ttrain-auc:0.82639\ttrain-logloss:0.50127\ttest-error:0.26925\ttest-auc:0.81369\ttest-logloss:0.51857\n",
      "[150]\ttrain-error:0.26381\ttrain-auc:0.83587\ttrain-logloss:0.48984\ttest-error:0.26950\ttest-auc:0.81400\ttest-logloss:0.51795\n",
      "[190]\ttrain-error:0.25675\ttrain-auc:0.84399\ttrain-logloss:0.48059\ttest-error:0.26850\ttest-auc:0.81340\ttest-logloss:0.51880\n",
      "Stopping after 192 rounds                                                         \n",
      "[0]\ttrain-error:0.19387\ttrain-auc:0.89671\ttrain-logloss:0.66393\ttest-error:0.32425\ttest-auc:0.75748\ttest-logloss:0.67498\n",
      "[50]\ttrain-error:0.01481\ttrain-auc:0.99671\ttrain-logloss:0.23509\ttest-error:0.28575\ttest-auc:0.80367\ttest-logloss:0.53042\n",
      "[97]\ttrain-error:0.00413\ttrain-auc:0.99893\ttrain-logloss:0.19425\ttest-error:0.28400\ttest-auc:0.80405\ttest-logloss:0.53543\n",
      "Stopping after 98 rounds                                                          \n",
      "[0]\ttrain-error:0.20650\ttrain-auc:0.88141\ttrain-logloss:0.67668\ttest-error:0.32550\ttest-auc:0.74053\ttest-logloss:0.68334\n",
      "[50]\ttrain-error:0.04194\ttrain-auc:0.99135\ttrain-logloss:0.33441\ttest-error:0.27600\ttest-auc:0.80863\ttest-logloss:0.53131\n",
      "[100]\ttrain-error:0.02081\ttrain-auc:0.99696\ttrain-logloss:0.25854\ttest-error:0.27800\ttest-auc:0.80830\ttest-logloss:0.52546\n",
      "[134]\ttrain-error:0.01394\ttrain-auc:0.99822\ttrain-logloss:0.24328\ttest-error:0.27750\ttest-auc:0.80828\ttest-logloss:0.52687\n",
      "Stopping after 135 rounds                                                         \n",
      "[0]\ttrain-error:0.27487\ttrain-auc:0.81156\ttrain-logloss:0.68663\ttest-error:0.30775\ttest-auc:0.77613\ttest-logloss:0.68734\n",
      "[50]\ttrain-error:0.24006\ttrain-auc:0.85661\ttrain-logloss:0.52628\ttest-error:0.27025\ttest-auc:0.81355\ttest-logloss:0.55339\n",
      "[100]\ttrain-error:0.23356\ttrain-auc:0.86586\ttrain-logloss:0.47874\ttest-error:0.26825\ttest-auc:0.81299\ttest-logloss:0.52655\n",
      "[150]\ttrain-error:0.22594\ttrain-auc:0.87478\ttrain-logloss:0.45699\ttest-error:0.26900\ttest-auc:0.81306\ttest-logloss:0.52042\n",
      "[200]\ttrain-error:0.22050\ttrain-auc:0.88166\ttrain-logloss:0.44451\ttest-error:0.26825\ttest-auc:0.81324\ttest-logloss:0.51876\n",
      "[249]\ttrain-error:0.21481\ttrain-auc:0.88986\ttrain-logloss:0.43354\ttest-error:0.26925\ttest-auc:0.81248\ttest-logloss:0.51937\n",
      "Stopping after 251 rounds                                                         \n",
      "[0]\ttrain-error:0.25594\ttrain-auc:0.82910\ttrain-logloss:0.63487\ttest-error:0.29425\ttest-auc:0.77916\ttest-logloss:0.64475\n",
      "[50]\ttrain-error:0.12269\ttrain-auc:0.95468\ttrain-logloss:0.34101\ttest-error:0.28250\ttest-auc:0.79698\ttest-logloss:0.54215\n",
      "[66]\ttrain-error:0.08313\ttrain-auc:0.97752\ttrain-logloss:0.29601\ttest-error:0.29250\ttest-auc:0.79098\ttest-logloss:0.55277\n",
      "Stopping after 68 rounds                                                          \n",
      "[0]\ttrain-error:0.27113\ttrain-auc:0.80986\ttrain-logloss:0.68039\ttest-error:0.27300\ttest-auc:0.80508\ttest-logloss:0.68057\n",
      "[50]\ttrain-error:0.26262\ttrain-auc:0.83023\ttrain-logloss:0.50737\ttest-error:0.26625\ttest-auc:0.81470\ttest-logloss:0.52324\n",
      "[100]\ttrain-error:0.25794\ttrain-auc:0.83840\ttrain-logloss:0.48814\ttest-error:0.26700\ttest-auc:0.81456\ttest-logloss:0.51699\n",
      "[150]\ttrain-error:0.25194\ttrain-auc:0.84882\ttrain-logloss:0.47493\ttest-error:0.26475\ttest-auc:0.81419\ttest-logloss:0.51652\n",
      "[183]\ttrain-error:0.24988\ttrain-auc:0.85536\ttrain-logloss:0.46682\ttest-error:0.26525\ttest-auc:0.81391\ttest-logloss:0.51709\n",
      "Stopping after 184 rounds                                                         \n",
      "[0]\ttrain-error:0.25038\ttrain-auc:0.84032\ttrain-logloss:0.65711\ttest-error:0.30025\ttest-auc:0.77722\ttest-logloss:0.66410\n",
      "[50]\ttrain-error:0.13656\ttrain-auc:0.94376\ttrain-logloss:0.37093\ttest-error:0.27925\ttest-auc:0.80673\ttest-logloss:0.52521\n",
      "[86]\ttrain-error:0.10069\ttrain-auc:0.96542\ttrain-logloss:0.33677\ttest-error:0.28425\ttest-auc:0.80466\ttest-logloss:0.52925\n",
      "Stopping after 87 rounds                                                          \n",
      "[0]\ttrain-error:0.27613\ttrain-auc:0.80771\ttrain-logloss:0.68645\ttest-error:0.26975\ttest-auc:0.80880\ttest-logloss:0.68644\n",
      "[50]\ttrain-error:0.27119\ttrain-auc:0.81616\ttrain-logloss:0.54504\ttest-error:0.27025\ttest-auc:0.81318\ttest-logloss:0.54718\n",
      "[100]\ttrain-error:0.26975\ttrain-auc:0.81917\ttrain-logloss:0.51737\ttest-error:0.26975\ttest-auc:0.81390\ttest-logloss:0.52335\n",
      "[150]\ttrain-error:0.26837\ttrain-auc:0.82198\ttrain-logloss:0.50854\ttest-error:0.26850\ttest-auc:0.81409\ttest-logloss:0.51863\n",
      "[200]\ttrain-error:0.26806\ttrain-auc:0.82426\ttrain-logloss:0.50415\ttest-error:0.26750\ttest-auc:0.81401\ttest-logloss:0.51766\n",
      "[250]\ttrain-error:0.26388\ttrain-auc:0.82781\ttrain-logloss:0.49976\ttest-error:0.27025\ttest-auc:0.81403\ttest-logloss:0.51721\n",
      "[288]\ttrain-error:0.26212\ttrain-auc:0.83057\ttrain-logloss:0.49649\ttest-error:0.26900\ttest-auc:0.81383\ttest-logloss:0.51724\n",
      "Stopping after 290 rounds                                                         \n",
      "[0]\ttrain-error:0.25588\ttrain-auc:0.83215\ttrain-logloss:0.66394\ttest-error:0.30125\ttest-auc:0.77918\ttest-logloss:0.66881\n",
      "[50]\ttrain-error:0.18800\ttrain-auc:0.90941\ttrain-logloss:0.41388\ttest-error:0.27350\ttest-auc:0.80975\ttest-logloss:0.52221\n",
      "[91]\ttrain-error:0.16381\ttrain-auc:0.92984\ttrain-logloss:0.38653\ttest-error:0.27525\ttest-auc:0.80894\ttest-logloss:0.52368\n",
      "Stopping after 92 rounds                                                          \n",
      "[0]\ttrain-error:0.27263\ttrain-auc:0.81143\ttrain-logloss:0.68498\ttest-error:0.27700\ttest-auc:0.80543\ttest-logloss:0.68511\n",
      "[50]\ttrain-error:0.25838\ttrain-auc:0.83019\ttrain-logloss:0.52711\ttest-error:0.27050\ttest-auc:0.81403\ttest-logloss:0.53864\n",
      "[100]\ttrain-error:0.25631\ttrain-auc:0.83571\ttrain-logloss:0.49965\ttest-error:0.26800\ttest-auc:0.81440\ttest-logloss:0.52060\n",
      "[150]\ttrain-error:0.25425\ttrain-auc:0.83939\ttrain-logloss:0.49117\ttest-error:0.27000\ttest-auc:0.81448\ttest-logloss:0.51726\n",
      "[200]\ttrain-error:0.25306\ttrain-auc:0.84180\ttrain-logloss:0.48673\ttest-error:0.26950\ttest-auc:0.81422\ttest-logloss:0.51691\n",
      "[250]\ttrain-error:0.25150\ttrain-auc:0.84521\ttrain-logloss:0.48194\ttest-error:0.26950\ttest-auc:0.81416\ttest-logloss:0.51660\n",
      "[300]\ttrain-error:0.24988\ttrain-auc:0.84848\ttrain-logloss:0.47758\ttest-error:0.26900\ttest-auc:0.81431\ttest-logloss:0.51639\n",
      "[342]\ttrain-error:0.24869\ttrain-auc:0.85122\ttrain-logloss:0.47429\ttest-error:0.26875\ttest-auc:0.81412\ttest-logloss:0.51646\n",
      "Stopping after 344 rounds                                                         \n",
      "[0]\ttrain-error:0.27094\ttrain-auc:0.80963\ttrain-logloss:0.68515\ttest-error:0.27025\ttest-auc:0.80598\ttest-logloss:0.68528\n",
      "[50]\ttrain-error:0.26600\ttrain-auc:0.82261\ttrain-logloss:0.53292\ttest-error:0.26650\ttest-auc:0.81390\ttest-logloss:0.53919\n",
      "[100]\ttrain-error:0.26400\ttrain-auc:0.82645\ttrain-logloss:0.50871\ttest-error:0.26650\ttest-auc:0.81397\ttest-logloss:0.52130\n",
      "[150]\ttrain-error:0.26162\ttrain-auc:0.82935\ttrain-logloss:0.50129\ttest-error:0.26675\ttest-auc:0.81394\ttest-logloss:0.51832\n",
      "[200]\ttrain-error:0.26019\ttrain-auc:0.83268\ttrain-logloss:0.49610\ttest-error:0.26775\ttest-auc:0.81390\ttest-logloss:0.51760\n",
      "[250]\ttrain-error:0.25950\ttrain-auc:0.83636\ttrain-logloss:0.49129\ttest-error:0.26775\ttest-auc:0.81407\ttest-logloss:0.51730\n",
      "[300]\ttrain-error:0.25862\ttrain-auc:0.83943\ttrain-logloss:0.48727\ttest-error:0.26850\ttest-auc:0.81399\ttest-logloss:0.51724\n",
      "[350]\ttrain-error:0.25581\ttrain-auc:0.84377\ttrain-logloss:0.48251\ttest-error:0.26775\ttest-auc:0.81400\ttest-logloss:0.51720\n",
      "[363]\ttrain-error:0.25481\ttrain-auc:0.84486\ttrain-logloss:0.48142\ttest-error:0.26900\ttest-auc:0.81396\ttest-logloss:0.51727\n",
      "Stopping after 364 rounds                                                         \n",
      "[0]\ttrain-error:0.27063\ttrain-auc:0.81170\ttrain-logloss:0.68408\ttest-error:0.27375\ttest-auc:0.80704\ttest-logloss:0.68426\n",
      "[50]\ttrain-error:0.26050\ttrain-auc:0.83327\ttrain-logloss:0.52008\ttest-error:0.27050\ttest-auc:0.81276\ttest-logloss:0.53570\n",
      "[100]\ttrain-error:0.25550\ttrain-auc:0.83889\ttrain-logloss:0.49459\ttest-error:0.27125\ttest-auc:0.81247\ttest-logloss:0.52109\n",
      "[150]\ttrain-error:0.25256\ttrain-auc:0.84346\ttrain-logloss:0.48575\ttest-error:0.27175\ttest-auc:0.81240\ttest-logloss:0.51924\n",
      "[200]\ttrain-error:0.24900\ttrain-auc:0.84945\ttrain-logloss:0.47773\ttest-error:0.27125\ttest-auc:0.81197\ttest-logloss:0.51944\n",
      "[226]\ttrain-error:0.24856\ttrain-auc:0.85139\ttrain-logloss:0.47496\ttest-error:0.27075\ttest-auc:0.81209\ttest-logloss:0.51934\n",
      "Stopping after 227 rounds                                                         \n",
      "[0]\ttrain-error:0.26969\ttrain-auc:0.81413\ttrain-logloss:0.68318\ttest-error:0.27450\ttest-auc:0.80196\ttest-logloss:0.68345\n",
      "[50]\ttrain-error:0.25512\ttrain-auc:0.83870\ttrain-logloss:0.50962\ttest-error:0.26950\ttest-auc:0.81337\ttest-logloss:0.53100\n",
      "[100]\ttrain-error:0.25038\ttrain-auc:0.84659\ttrain-logloss:0.48364\ttest-error:0.26875\ttest-auc:0.81325\ttest-logloss:0.51879\n",
      "[150]\ttrain-error:0.24488\ttrain-auc:0.85290\ttrain-logloss:0.47379\ttest-error:0.26975\ttest-auc:0.81286\ttest-logloss:0.51820\n",
      "[173]\ttrain-error:0.24381\ttrain-auc:0.85611\ttrain-logloss:0.46968\ttest-error:0.26975\ttest-auc:0.81273\ttest-logloss:0.51817\n",
      "Stopping after 175 rounds                                                         \n",
      "[0]\ttrain-error:0.27587\ttrain-auc:0.80633\ttrain-logloss:0.68491\ttest-error:0.27125\ttest-auc:0.80656\ttest-logloss:0.68488\n",
      "[50]\ttrain-error:0.27063\ttrain-auc:0.81656\ttrain-logloss:0.53443\ttest-error:0.27075\ttest-auc:0.81382\ttest-logloss:0.53719\n",
      "[100]\ttrain-error:0.26887\ttrain-auc:0.82044\ttrain-logloss:0.51179\ttest-error:0.26775\ttest-auc:0.81397\ttest-logloss:0.52003\n",
      "[150]\ttrain-error:0.26619\ttrain-auc:0.82378\ttrain-logloss:0.50450\ttest-error:0.26700\ttest-auc:0.81401\ttest-logloss:0.51756\n",
      "[200]\ttrain-error:0.26512\ttrain-auc:0.82703\ttrain-logloss:0.49925\ttest-error:0.26875\ttest-auc:0.81437\ttest-logloss:0.51677\n",
      "[250]\ttrain-error:0.26381\ttrain-auc:0.83159\ttrain-logloss:0.49363\ttest-error:0.26925\ttest-auc:0.81459\ttest-logloss:0.51652\n",
      "[286]\ttrain-error:0.26212\ttrain-auc:0.83486\ttrain-logloss:0.48971\ttest-error:0.26900\ttest-auc:0.81427\ttest-logloss:0.51669\n",
      "Stopping after 287 rounds                                                         \n",
      "[0]\ttrain-error:0.26719\ttrain-auc:0.81828\ttrain-logloss:0.68176\ttest-error:0.29450\ttest-auc:0.79532\ttest-logloss:0.68252\n",
      "[50]\ttrain-error:0.24088\ttrain-auc:0.85432\ttrain-logloss:0.49240\ttest-error:0.27000\ttest-auc:0.81456\ttest-logloss:0.52718\n",
      "[100]\ttrain-error:0.23419\ttrain-auc:0.86474\ttrain-logloss:0.46468\ttest-error:0.26775\ttest-auc:0.81393\ttest-logloss:0.51810\n",
      "[150]\ttrain-error:0.22744\ttrain-auc:0.87411\ttrain-logloss:0.45172\ttest-error:0.26575\ttest-auc:0.81430\ttest-logloss:0.51698\n",
      "[195]\ttrain-error:0.21981\ttrain-auc:0.88458\ttrain-logloss:0.43936\ttest-error:0.26625\ttest-auc:0.81384\ttest-logloss:0.51754\n",
      "Stopping after 197 rounds                                                         \n",
      "[0]\ttrain-error:0.27469\ttrain-auc:0.80734\ttrain-logloss:0.68750\ttest-error:0.27000\ttest-auc:0.80844\ttest-logloss:0.68746\n",
      "[50]\ttrain-error:0.27181\ttrain-auc:0.81559\ttrain-logloss:0.55476\ttest-error:0.26675\ttest-auc:0.81390\ttest-logloss:0.55608\n",
      "[100]\ttrain-error:0.27006\ttrain-auc:0.81789\ttrain-logloss:0.52323\ttest-error:0.26625\ttest-auc:0.81442\ttest-logloss:0.52725\n",
      "[150]\ttrain-error:0.26925\ttrain-auc:0.82020\ttrain-logloss:0.51276\ttest-error:0.26575\ttest-auc:0.81479\ttest-logloss:0.51982\n",
      "[200]\ttrain-error:0.26844\ttrain-auc:0.82207\ttrain-logloss:0.50797\ttest-error:0.26575\ttest-auc:0.81481\ttest-logloss:0.51767\n",
      "[250]\ttrain-error:0.26800\ttrain-auc:0.82397\ttrain-logloss:0.50448\ttest-error:0.26700\ttest-auc:0.81488\ttest-logloss:0.51678\n",
      "[300]\ttrain-error:0.26713\ttrain-auc:0.82703\ttrain-logloss:0.50067\ttest-error:0.26825\ttest-auc:0.81480\ttest-logloss:0.51651\n",
      "[350]\ttrain-error:0.26562\ttrain-auc:0.82956\ttrain-logloss:0.49733\ttest-error:0.26775\ttest-auc:0.81460\ttest-logloss:0.51653\n",
      "[387]\ttrain-error:0.26400\ttrain-auc:0.83182\ttrain-logloss:0.49447\ttest-error:0.26775\ttest-auc:0.81447\ttest-logloss:0.51653\n",
      "Stopping after 389 rounds                                                         \n",
      "[0]\ttrain-error:0.27837\ttrain-auc:0.80581\ttrain-logloss:0.68681\ttest-error:0.27100\ttest-auc:0.80551\ttest-logloss:0.68683\n",
      "[50]\ttrain-error:0.27206\ttrain-auc:0.81635\ttrain-logloss:0.54759\ttest-error:0.26775\ttest-auc:0.81375\ttest-logloss:0.54959\n",
      "[100]\ttrain-error:0.27000\ttrain-auc:0.81949\ttrain-logloss:0.51834\ttest-error:0.26600\ttest-auc:0.81408\ttest-logloss:0.52431\n",
      "[150]\ttrain-error:0.26863\ttrain-auc:0.82200\ttrain-logloss:0.50885\ttest-error:0.26850\ttest-auc:0.81447\ttest-logloss:0.51852\n",
      "[200]\ttrain-error:0.26800\ttrain-auc:0.82479\ttrain-logloss:0.50358\ttest-error:0.26925\ttest-auc:0.81449\ttest-logloss:0.51710\n",
      "[250]\ttrain-error:0.26731\ttrain-auc:0.82725\ttrain-logloss:0.49955\ttest-error:0.26975\ttest-auc:0.81482\ttest-logloss:0.51635\n",
      "[292]\ttrain-error:0.26575\ttrain-auc:0.83069\ttrain-logloss:0.49565\ttest-error:0.27150\ttest-auc:0.81451\ttest-logloss:0.51658\n",
      "Stopping after 294 rounds                                                         \n",
      "[0]\ttrain-error:0.27837\ttrain-auc:0.80571\ttrain-logloss:0.68684\ttest-error:0.27100\ttest-auc:0.80521\ttest-logloss:0.68685\n",
      "[50]\ttrain-error:0.27156\ttrain-auc:0.81622\ttrain-logloss:0.54795\ttest-error:0.26750\ttest-auc:0.81355\ttest-logloss:0.54995\n",
      "[100]\ttrain-error:0.27037\ttrain-auc:0.81913\ttrain-logloss:0.51897\ttest-error:0.27000\ttest-auc:0.81382\ttest-logloss:0.52464\n",
      "[150]\ttrain-error:0.26800\ttrain-auc:0.82173\ttrain-logloss:0.50957\ttest-error:0.27000\ttest-auc:0.81405\ttest-logloss:0.51895\n",
      "[200]\ttrain-error:0.26694\ttrain-auc:0.82443\ttrain-logloss:0.50456\ttest-error:0.27100\ttest-auc:0.81400\ttest-logloss:0.51770\n",
      "[250]\ttrain-error:0.26519\ttrain-auc:0.82727\ttrain-logloss:0.50041\ttest-error:0.27050\ttest-auc:0.81415\ttest-logloss:0.51711\n",
      "[300]\ttrain-error:0.26356\ttrain-auc:0.83114\ttrain-logloss:0.49580\ttest-error:0.26975\ttest-auc:0.81401\ttest-logloss:0.51703\n",
      "[350]\ttrain-error:0.26275\ttrain-auc:0.83471\ttrain-logloss:0.49127\ttest-error:0.26950\ttest-auc:0.81407\ttest-logloss:0.51680\n",
      "[400]\ttrain-error:0.25962\ttrain-auc:0.83872\ttrain-logloss:0.48656\ttest-error:0.27000\ttest-auc:0.81388\ttest-logloss:0.51698\n",
      "Stopping after 401 rounds                                                         \n",
      "[0]\ttrain-error:0.27169\ttrain-auc:0.80894\ttrain-logloss:0.68778\ttest-error:0.27925\ttest-auc:0.80412\ttest-logloss:0.68784\n",
      "[50]\ttrain-error:0.26856\ttrain-auc:0.82399\ttrain-logloss:0.55390\ttest-error:0.26750\ttest-auc:0.81318\ttest-logloss:0.56002\n",
      "[100]\ttrain-error:0.26494\ttrain-auc:0.82754\ttrain-logloss:0.51703\ttest-error:0.26800\ttest-auc:0.81389\ttest-logloss:0.52904\n",
      "[150]\ttrain-error:0.26319\ttrain-auc:0.83065\ttrain-logloss:0.50338\ttest-error:0.26675\ttest-auc:0.81400\ttest-logloss:0.52066\n",
      "[200]\ttrain-error:0.26175\ttrain-auc:0.83310\ttrain-logloss:0.49698\ttest-error:0.26625\ttest-auc:0.81423\ttest-logloss:0.51802\n",
      "[250]\ttrain-error:0.25956\ttrain-auc:0.83670\ttrain-logloss:0.49170\ttest-error:0.26625\ttest-auc:0.81424\ttest-logloss:0.51709\n",
      "[300]\ttrain-error:0.25719\ttrain-auc:0.84037\ttrain-logloss:0.48696\ttest-error:0.26500\ttest-auc:0.81388\ttest-logloss:0.51720\n",
      "[350]\ttrain-error:0.25506\ttrain-auc:0.84472\ttrain-logloss:0.48204\ttest-error:0.26500\ttest-auc:0.81399\ttest-logloss:0.51688\n",
      "[393]\ttrain-error:0.25175\ttrain-auc:0.84866\ttrain-logloss:0.47799\ttest-error:0.26775\ttest-auc:0.81386\ttest-logloss:0.51695\n",
      "Stopping after 394 rounds                                                         \n",
      "[0]\ttrain-error:0.27525\ttrain-auc:0.80694\ttrain-logloss:0.68590\ttest-error:0.27625\ttest-auc:0.80703\ttest-logloss:0.68588\n",
      "[50]\ttrain-error:0.27263\ttrain-auc:0.81653\ttrain-logloss:0.54037\ttest-error:0.26900\ttest-auc:0.81373\ttest-logloss:0.54283\n",
      "[100]\ttrain-error:0.27000\ttrain-auc:0.81999\ttrain-logloss:0.51425\ttest-error:0.26850\ttest-auc:0.81419\ttest-logloss:0.52139\n",
      "[150]\ttrain-error:0.26819\ttrain-auc:0.82304\ttrain-logloss:0.50593\ttest-error:0.26825\ttest-auc:0.81428\ttest-logloss:0.51768\n",
      "[200]\ttrain-error:0.26731\ttrain-auc:0.82561\ttrain-logloss:0.50115\ttest-error:0.26825\ttest-auc:0.81413\ttest-logloss:0.51711\n",
      "[250]\ttrain-error:0.26625\ttrain-auc:0.82908\ttrain-logloss:0.49638\ttest-error:0.26850\ttest-auc:0.81432\ttest-logloss:0.51669\n",
      "[300]\ttrain-error:0.26344\ttrain-auc:0.83326\ttrain-logloss:0.49108\ttest-error:0.27075\ttest-auc:0.81429\ttest-logloss:0.51665\n",
      "[327]\ttrain-error:0.26225\ttrain-auc:0.83522\ttrain-logloss:0.48854\ttest-error:0.27075\ttest-auc:0.81430\ttest-logloss:0.51671\n",
      "Stopping after 328 rounds                                                         \n",
      "[0]\ttrain-error:0.26725\ttrain-auc:0.81783\ttrain-logloss:0.67441\ttest-error:0.28950\ttest-auc:0.79199\ttest-logloss:0.67611\n",
      "[50]\ttrain-error:0.23181\ttrain-auc:0.87125\ttrain-logloss:0.45979\ttest-error:0.27075\ttest-auc:0.81184\ttest-logloss:0.52124\n",
      "[100]\ttrain-error:0.20694\ttrain-auc:0.89501\ttrain-logloss:0.42667\ttest-error:0.27375\ttest-auc:0.81014\ttest-logloss:0.52166\n",
      "[136]\ttrain-error:0.18200\ttrain-auc:0.91613\ttrain-logloss:0.40258\ttest-error:0.27550\ttest-auc:0.80908\ttest-logloss:0.52336\n",
      "Stopping after 138 rounds                                                         \n",
      "[0]\ttrain-error:0.25519\ttrain-auc:0.83129\ttrain-logloss:0.66748\ttest-error:0.29800\ttest-auc:0.78287\ttest-logloss:0.67143\n",
      "[50]\ttrain-error:0.15894\ttrain-auc:0.93114\ttrain-logloss:0.39091\ttest-error:0.27900\ttest-auc:0.80959\ttest-logloss:0.52122\n",
      "[100]\ttrain-error:0.11675\ttrain-auc:0.96139\ttrain-logloss:0.34499\ttest-error:0.28075\ttest-auc:0.80885\ttest-logloss:0.52378\n",
      "[105]\ttrain-error:0.10756\ttrain-auc:0.96557\ttrain-logloss:0.33804\ttest-error:0.28225\ttest-auc:0.80876\ttest-logloss:0.52399\n",
      "Stopping after 107 rounds                                                         \n",
      "[0]\ttrain-error:0.25481\ttrain-auc:0.83656\ttrain-logloss:0.67902\ttest-error:0.28550\ttest-auc:0.78519\ttest-logloss:0.68117\n",
      "[50]\ttrain-error:0.22225\ttrain-auc:0.88101\ttrain-logloss:0.46161\ttest-error:0.26725\ttest-auc:0.81261\ttest-logloss:0.52513\n",
      "[100]\ttrain-error:0.20581\ttrain-auc:0.89623\ttrain-logloss:0.42994\ttest-error:0.26500\ttest-auc:0.81257\ttest-logloss:0.51888\n",
      "[150]\ttrain-error:0.19894\ttrain-auc:0.90624\ttrain-logloss:0.41594\ttest-error:0.26650\ttest-auc:0.81202\ttest-logloss:0.51957\n",
      "[151]\ttrain-error:0.19887\ttrain-auc:0.90631\ttrain-logloss:0.41583\ttest-error:0.26650\ttest-auc:0.81203\ttest-logloss:0.51954\n",
      "Stopping after 153 rounds                                                         \n",
      "[0]\ttrain-error:0.27500\ttrain-auc:0.80893\ttrain-logloss:0.68612\ttest-error:0.27275\ttest-auc:0.80535\ttest-logloss:0.68623\n",
      "[50]\ttrain-error:0.26587\ttrain-auc:0.82461\ttrain-logloss:0.53792\ttest-error:0.27125\ttest-auc:0.81390\ttest-logloss:0.54530\n",
      "[100]\ttrain-error:0.26350\ttrain-auc:0.82922\ttrain-logloss:0.50790\ttest-error:0.27000\ttest-auc:0.81441\ttest-logloss:0.52246\n",
      "[150]\ttrain-error:0.25888\ttrain-auc:0.83300\ttrain-logloss:0.49786\ttest-error:0.26950\ttest-auc:0.81440\ttest-logloss:0.51812\n",
      "[200]\ttrain-error:0.25844\ttrain-auc:0.83648\ttrain-logloss:0.49190\ttest-error:0.27000\ttest-auc:0.81433\ttest-logloss:0.51707\n",
      "[250]\ttrain-error:0.25481\ttrain-auc:0.84196\ttrain-logloss:0.48517\ttest-error:0.27025\ttest-auc:0.81431\ttest-logloss:0.51692\n",
      "[282]\ttrain-error:0.25388\ttrain-auc:0.84448\ttrain-logloss:0.48162\ttest-error:0.26875\ttest-auc:0.81432\ttest-logloss:0.51674\n",
      "Stopping after 283 rounds                                                         \n",
      "[0]\ttrain-error:0.27669\ttrain-auc:0.80353\ttrain-logloss:0.68390\ttest-error:0.29025\ttest-auc:0.77997\ttest-logloss:0.68458\n",
      "[50]\ttrain-error:0.24356\ttrain-auc:0.85029\ttrain-logloss:0.50774\ttest-error:0.27400\ttest-auc:0.81110\ttest-logloss:0.53662\n",
      "[100]\ttrain-error:0.23150\ttrain-auc:0.86085\ttrain-logloss:0.47465\ttest-error:0.27200\ttest-auc:0.81115\ttest-logloss:0.52223\n",
      "[150]\ttrain-error:0.22300\ttrain-auc:0.87027\ttrain-logloss:0.45994\ttest-error:0.27400\ttest-auc:0.81036\ttest-logloss:0.52176\n",
      "[200]\ttrain-error:0.20888\ttrain-auc:0.88270\ttrain-logloss:0.44592\ttest-error:0.27800\ttest-auc:0.80948\ttest-logloss:0.52239\n",
      "[214]\ttrain-error:0.20444\ttrain-auc:0.88614\ttrain-logloss:0.44190\ttest-error:0.27725\ttest-auc:0.80956\ttest-logloss:0.52252\n",
      "Stopping after 215 rounds                                                         \n",
      "[0]\ttrain-error:0.25394\ttrain-auc:0.83463\ttrain-logloss:0.68539\ttest-error:0.30400\ttest-auc:0.76933\ttest-logloss:0.68700\n",
      "[50]\ttrain-error:0.16987\ttrain-auc:0.92354\ttrain-logloss:0.47817\ttest-error:0.27200\ttest-auc:0.81001\ttest-logloss:0.55251\n",
      "[100]\ttrain-error:0.14463\ttrain-auc:0.93982\ttrain-logloss:0.40700\ttest-error:0.27100\ttest-auc:0.81035\ttest-logloss:0.52669\n",
      "[150]\ttrain-error:0.12700\ttrain-auc:0.95115\ttrain-logloss:0.37379\ttest-error:0.27050\ttest-auc:0.81143\ttest-logloss:0.52104\n",
      "[200]\ttrain-error:0.11519\ttrain-auc:0.95903\ttrain-logloss:0.35749\ttest-error:0.27075\ttest-auc:0.81153\ttest-logloss:0.51998\n",
      "[250]\ttrain-error:0.10287\ttrain-auc:0.96726\ttrain-logloss:0.34296\ttest-error:0.27000\ttest-auc:0.81098\ttest-logloss:0.52097\n",
      "[254]\ttrain-error:0.10212\ttrain-auc:0.96782\ttrain-logloss:0.34206\ttest-error:0.27150\ttest-auc:0.81097\ttest-logloss:0.52105\n",
      "Stopping after 256 rounds                                                         \n",
      "[0]\ttrain-error:0.26925\ttrain-auc:0.81486\ttrain-logloss:0.68537\ttest-error:0.28225\ttest-auc:0.80492\ttest-logloss:0.68565\n",
      "[50]\ttrain-error:0.25712\ttrain-auc:0.83861\ttrain-logloss:0.52305\ttest-error:0.27075\ttest-auc:0.81465\ttest-logloss:0.54077\n",
      "[100]\ttrain-error:0.25294\ttrain-auc:0.84466\ttrain-logloss:0.49010\ttest-error:0.26900\ttest-auc:0.81436\ttest-logloss:0.52056\n",
      "[150]\ttrain-error:0.24700\ttrain-auc:0.85004\ttrain-logloss:0.47857\ttest-error:0.26975\ttest-auc:0.81395\ttest-logloss:0.51790\n",
      "[200]\ttrain-error:0.24431\ttrain-auc:0.85420\ttrain-logloss:0.47193\ttest-error:0.27000\ttest-auc:0.81394\ttest-logloss:0.51740\n",
      "[250]\ttrain-error:0.24088\ttrain-auc:0.86110\ttrain-logloss:0.46426\ttest-error:0.26775\ttest-auc:0.81397\ttest-logloss:0.51734\n",
      "[300]\ttrain-error:0.23688\ttrain-auc:0.86642\ttrain-logloss:0.45807\ttest-error:0.27125\ttest-auc:0.81375\ttest-logloss:0.51769\n",
      "[303]\ttrain-error:0.23625\ttrain-auc:0.86669\ttrain-logloss:0.45771\ttest-error:0.27075\ttest-auc:0.81365\ttest-logloss:0.51774\n",
      "Stopping after 305 rounds                                                         \n",
      "[0]\ttrain-error:0.27444\ttrain-auc:0.80478\ttrain-logloss:0.67741\ttest-error:0.27300\ttest-auc:0.80737\ttest-logloss:0.67734\n",
      "[50]\ttrain-error:0.27194\ttrain-auc:0.81711\ttrain-logloss:0.51672\ttest-error:0.27100\ttest-auc:0.81415\ttest-logloss:0.52102\n",
      "[100]\ttrain-error:0.26881\ttrain-auc:0.82132\ttrain-logloss:0.50771\ttest-error:0.26950\ttest-auc:0.81406\ttest-logloss:0.51773\n",
      "[150]\ttrain-error:0.26594\ttrain-auc:0.82659\ttrain-logloss:0.50103\ttest-error:0.27025\ttest-auc:0.81438\ttest-logloss:0.51716\n",
      "[200]\ttrain-error:0.26256\ttrain-auc:0.83278\ttrain-logloss:0.49410\ttest-error:0.26900\ttest-auc:0.81375\ttest-logloss:0.51789\n",
      "[205]\ttrain-error:0.26219\ttrain-auc:0.83357\ttrain-logloss:0.49326\ttest-error:0.26875\ttest-auc:0.81390\ttest-logloss:0.51785\n",
      "Stopping after 207 rounds                                                         \n",
      "[0]\ttrain-error:0.26937\ttrain-auc:0.81304\ttrain-logloss:0.68082\ttest-error:0.27600\ttest-auc:0.79607\ttest-logloss:0.68144\n",
      "[50]\ttrain-error:0.24781\ttrain-auc:0.85154\ttrain-logloss:0.49220\ttest-error:0.26675\ttest-auc:0.81364\ttest-logloss:0.52550\n",
      "[100]\ttrain-error:0.23963\ttrain-auc:0.86197\ttrain-logloss:0.46745\ttest-error:0.26625\ttest-auc:0.81347\ttest-logloss:0.51828\n",
      "[150]\ttrain-error:0.22419\ttrain-auc:0.87567\ttrain-logloss:0.45059\ttest-error:0.26775\ttest-auc:0.81294\ttest-logloss:0.51858\n",
      "[171]\ttrain-error:0.21744\ttrain-auc:0.88107\ttrain-logloss:0.44433\ttest-error:0.26925\ttest-auc:0.81280\ttest-logloss:0.51881\n",
      "Stopping after 172 rounds                                                         \n",
      "[0]\ttrain-error:0.25550\ttrain-auc:0.83024\ttrain-logloss:0.68100\ttest-error:0.30750\ttest-auc:0.77033\ttest-logloss:0.68326\n",
      "[50]\ttrain-error:0.15931\ttrain-auc:0.91936\ttrain-logloss:0.43888\ttest-error:0.27050\ttest-auc:0.81052\ttest-logloss:0.53085\n",
      "[100]\ttrain-error:0.12412\ttrain-auc:0.94610\ttrain-logloss:0.37550\ttest-error:0.27550\ttest-auc:0.81001\ttest-logloss:0.52202\n",
      "[150]\ttrain-error:0.10519\ttrain-auc:0.95976\ttrain-logloss:0.34946\ttest-error:0.27325\ttest-auc:0.80965\ttest-logloss:0.52226\n",
      "[169]\ttrain-error:0.09419\ttrain-auc:0.96488\ttrain-logloss:0.33998\ttest-error:0.27850\ttest-auc:0.80891\ttest-logloss:0.52317\n",
      "Stopping after 171 rounds                                                         \n",
      "[0]\ttrain-error:0.27356\ttrain-auc:0.81058\ttrain-logloss:0.68364\ttest-error:0.27075\ttest-auc:0.80703\ttest-logloss:0.68382\n",
      "[50]\ttrain-error:0.26256\ttrain-auc:0.82844\ttrain-logloss:0.51994\ttest-error:0.26975\ttest-auc:0.81354\ttest-logloss:0.53255\n",
      "[100]\ttrain-error:0.25969\ttrain-auc:0.83444\ttrain-logloss:0.49537\ttest-error:0.27050\ttest-auc:0.81416\ttest-logloss:0.51844\n",
      "[150]\ttrain-error:0.25400\ttrain-auc:0.84140\ttrain-logloss:0.48479\ttest-error:0.26975\ttest-auc:0.81443\ttest-logloss:0.51669\n",
      "[200]\ttrain-error:0.25094\ttrain-auc:0.84650\ttrain-logloss:0.47694\ttest-error:0.26950\ttest-auc:0.81457\ttest-logloss:0.51627\n",
      "[232]\ttrain-error:0.24844\ttrain-auc:0.85230\ttrain-logloss:0.47039\ttest-error:0.27075\ttest-auc:0.81393\ttest-logloss:0.51684\n",
      "Stopping after 233 rounds                                                         \n",
      "[0]\ttrain-error:0.26369\ttrain-auc:0.81779\ttrain-logloss:0.67810\ttest-error:0.28900\ttest-auc:0.79298\ttest-logloss:0.67928\n",
      "[50]\ttrain-error:0.24588\ttrain-auc:0.84890\ttrain-logloss:0.48918\ttest-error:0.26825\ttest-auc:0.81439\ttest-logloss:0.52227\n",
      "[100]\ttrain-error:0.24175\ttrain-auc:0.85613\ttrain-logloss:0.47264\ttest-error:0.26625\ttest-auc:0.81395\ttest-logloss:0.51824\n",
      "[150]\ttrain-error:0.23463\ttrain-auc:0.86394\ttrain-logloss:0.46251\ttest-error:0.26675\ttest-auc:0.81401\ttest-logloss:0.51808\n",
      "[159]\ttrain-error:0.23300\ttrain-auc:0.86621\ttrain-logloss:0.45968\ttest-error:0.26650\ttest-auc:0.81373\ttest-logloss:0.51839\n",
      "Stopping after 160 rounds                                                         \n",
      "[0]\ttrain-error:0.27100\ttrain-auc:0.81598\ttrain-logloss:0.68323\ttest-error:0.27525\ttest-auc:0.80453\ttest-logloss:0.68353\n",
      "[50]\ttrain-error:0.25281\ttrain-auc:0.84420\ttrain-logloss:0.50680\ttest-error:0.27250\ttest-auc:0.81396\ttest-logloss:0.53138\n",
      "[100]\ttrain-error:0.24525\ttrain-auc:0.85454\ttrain-logloss:0.47611\ttest-error:0.27125\ttest-auc:0.81432\ttest-logloss:0.51811\n",
      "[150]\ttrain-error:0.24013\ttrain-auc:0.86259\ttrain-logloss:0.46321\ttest-error:0.27525\ttest-auc:0.81391\ttest-logloss:0.51728\n",
      "[193]\ttrain-error:0.23488\ttrain-auc:0.87063\ttrain-logloss:0.45257\ttest-error:0.27225\ttest-auc:0.81336\ttest-logloss:0.51806\n",
      "Stopping after 195 rounds                                                         \n",
      "[0]\ttrain-error:0.27506\ttrain-auc:0.80743\ttrain-logloss:0.66636\ttest-error:0.28150\ttest-auc:0.80331\ttest-logloss:0.66687\n",
      "[50]\ttrain-error:0.25562\ttrain-auc:0.84073\ttrain-logloss:0.48565\ttest-error:0.26750\ttest-auc:0.81238\ttest-logloss:0.51951\n",
      "[99]\ttrain-error:0.23112\ttrain-auc:0.87187\ttrain-logloss:0.44922\ttest-error:0.27375\ttest-auc:0.81034\ttest-logloss:0.52166\n",
      "Stopping after 100 rounds                                                         \n",
      "[0]\ttrain-error:0.22919\ttrain-auc:0.86108\ttrain-logloss:0.66566\ttest-error:0.31475\ttest-auc:0.76381\ttest-logloss:0.67347\n",
      "[50]\ttrain-error:0.09725\ttrain-auc:0.96530\ttrain-logloss:0.34446\ttest-error:0.28025\ttest-auc:0.80722\ttest-logloss:0.52543\n",
      "[100]\ttrain-error:0.08313\ttrain-auc:0.97570\ttrain-logloss:0.32321\ttest-error:0.27700\ttest-auc:0.80717\ttest-logloss:0.52644\n",
      "[114]\ttrain-error:0.07662\ttrain-auc:0.97888\ttrain-logloss:0.31660\ttest-error:0.27700\ttest-auc:0.80691\ttest-logloss:0.52683\n",
      "Stopping after 116 rounds                                                         \n",
      "[0]\ttrain-error:0.23312\ttrain-auc:0.85724\ttrain-logloss:0.68529\ttest-error:0.30800\ttest-auc:0.77536\ttest-logloss:0.68719\n",
      "[50]\ttrain-error:0.14894\ttrain-auc:0.93633\ttrain-logloss:0.47611\ttest-error:0.27575\ttest-auc:0.81135\ttest-logloss:0.55485\n",
      "[100]\ttrain-error:0.12344\ttrain-auc:0.95231\ttrain-logloss:0.39891\ttest-error:0.27325\ttest-auc:0.81067\ttest-logloss:0.52782\n",
      "[150]\ttrain-error:0.10325\ttrain-auc:0.96282\ttrain-logloss:0.36170\ttest-error:0.27325\ttest-auc:0.81092\ttest-logloss:0.52159\n",
      "[200]\ttrain-error:0.09050\ttrain-auc:0.96975\ttrain-logloss:0.34305\ttest-error:0.27375\ttest-auc:0.81075\ttest-logloss:0.52069\n",
      "[250]\ttrain-error:0.08138\ttrain-auc:0.97535\ttrain-logloss:0.33068\ttest-error:0.27375\ttest-auc:0.81094\ttest-logloss:0.52046\n",
      "[293]\ttrain-error:0.07356\ttrain-auc:0.98005\ttrain-logloss:0.32066\ttest-error:0.27550\ttest-auc:0.81068\ttest-logloss:0.52103\n",
      "Stopping after 294 rounds                                                         \n",
      "[0]\ttrain-error:0.26288\ttrain-auc:0.82798\ttrain-logloss:0.67880\ttest-error:0.28325\ttest-auc:0.79525\ttest-logloss:0.68027\n",
      "[50]\ttrain-error:0.22194\ttrain-auc:0.88283\ttrain-logloss:0.45685\ttest-error:0.26775\ttest-auc:0.81213\ttest-logloss:0.52418\n",
      "[100]\ttrain-error:0.20287\ttrain-auc:0.90496\ttrain-logloss:0.41601\ttest-error:0.27175\ttest-auc:0.81285\ttest-logloss:0.51901\n",
      "[150]\ttrain-error:0.17825\ttrain-auc:0.92778\ttrain-logloss:0.38864\ttest-error:0.27450\ttest-auc:0.81181\ttest-logloss:0.52059\n",
      "[161]\ttrain-error:0.16919\ttrain-auc:0.93406\ttrain-logloss:0.38127\ttest-error:0.27325\ttest-auc:0.81208\ttest-logloss:0.52043\n",
      "Stopping after 162 rounds                                                         \n",
      "[0]\ttrain-error:0.26950\ttrain-auc:0.80792\ttrain-logloss:0.65690\ttest-error:0.28900\ttest-auc:0.79886\ttest-logloss:0.65824\n",
      "[50]\ttrain-error:0.24238\ttrain-auc:0.84871\ttrain-logloss:0.47954\ttest-error:0.27175\ttest-auc:0.81102\ttest-logloss:0.52105\n",
      "[91]\ttrain-error:0.22681\ttrain-auc:0.86611\ttrain-logloss:0.45968\ttest-error:0.27625\ttest-auc:0.80759\ttest-logloss:0.52360\n",
      "Stopping after 93 rounds                                                          \n",
      "[0]\ttrain-error:0.21100\ttrain-auc:0.87622\ttrain-logloss:0.68329\ttest-error:0.31425\ttest-auc:0.75774\ttest-logloss:0.68661\n",
      "[50]\ttrain-error:0.07487\ttrain-auc:0.97870\ttrain-logloss:0.41985\ttest-error:0.27475\ttest-auc:0.80829\ttest-logloss:0.55066\n",
      "[100]\ttrain-error:0.04631\ttrain-auc:0.98982\ttrain-logloss:0.32395\ttest-error:0.27525\ttest-auc:0.80920\ttest-logloss:0.52680\n",
      "[150]\ttrain-error:0.03281\ttrain-auc:0.99416\ttrain-logloss:0.27982\ttest-error:0.27700\ttest-auc:0.80896\ttest-logloss:0.52378\n",
      "[192]\ttrain-error:0.02231\ttrain-auc:0.99644\ttrain-logloss:0.26321\ttest-error:0.27475\ttest-auc:0.80902\ttest-logloss:0.52395\n",
      "Stopping after 194 rounds                                                         \n",
      "[0]\ttrain-error:0.27125\ttrain-auc:0.81063\ttrain-logloss:0.68396\ttest-error:0.27600\ttest-auc:0.80496\ttest-logloss:0.68409\n",
      "[50]\ttrain-error:0.26369\ttrain-auc:0.82777\ttrain-logloss:0.52149\ttest-error:0.26975\ttest-auc:0.81392\ttest-logloss:0.53314\n",
      "[100]\ttrain-error:0.25969\ttrain-auc:0.83391\ttrain-logloss:0.49560\ttest-error:0.27100\ttest-auc:0.81378\ttest-logloss:0.51915\n",
      "[150]\ttrain-error:0.25744\ttrain-auc:0.83933\ttrain-logloss:0.48518\ttest-error:0.26975\ttest-auc:0.81367\ttest-logloss:0.51763\n",
      "[187]\ttrain-error:0.25444\ttrain-auc:0.84479\ttrain-logloss:0.47790\ttest-error:0.27000\ttest-auc:0.81344\ttest-logloss:0.51772\n",
      "Stopping after 189 rounds                                                         \n",
      "[0]\ttrain-error:0.27319\ttrain-auc:0.81043\ttrain-logloss:0.67208\ttest-error:0.30750\ttest-auc:0.77301\ttest-logloss:0.67484\n",
      "[50]\ttrain-error:0.21119\ttrain-auc:0.88384\ttrain-logloss:0.44464\ttest-error:0.27675\ttest-auc:0.80958\ttest-logloss:0.52285\n",
      "[100]\ttrain-error:0.18200\ttrain-auc:0.91459\ttrain-logloss:0.40324\ttest-error:0.27625\ttest-auc:0.80707\ttest-logloss:0.52463\n",
      "[103]\ttrain-error:0.17850\ttrain-auc:0.91715\ttrain-logloss:0.40045\ttest-error:0.27725\ttest-auc:0.80731\ttest-logloss:0.52441\n",
      "Stopping after 104 rounds                                                         \n",
      "[0]\ttrain-error:0.26038\ttrain-auc:0.82469\ttrain-logloss:0.61011\ttest-error:0.32450\ttest-auc:0.76011\ttest-logloss:0.63129\n",
      "[50]\ttrain-error:0.04775\ttrain-auc:0.98963\ttrain-logloss:0.21715\ttest-error:0.30500\ttest-auc:0.77200\ttest-logloss:0.62258\n",
      "[56]\ttrain-error:0.03969\ttrain-auc:0.99232\ttrain-logloss:0.20567\ttest-error:0.30450\ttest-auc:0.77072\ttest-logloss:0.62704\n",
      "Stopping after 58 rounds                                                          \n",
      "[0]\ttrain-error:0.25119\ttrain-auc:0.83963\ttrain-logloss:0.68656\ttest-error:0.29450\ttest-auc:0.77794\ttest-logloss:0.68781\n",
      "[50]\ttrain-error:0.20525\ttrain-auc:0.90279\ttrain-logloss:0.50428\ttest-error:0.26825\ttest-auc:0.81374\ttest-logloss:0.55768\n",
      "[100]\ttrain-error:0.18894\ttrain-auc:0.91588\ttrain-logloss:0.43859\ttest-error:0.26850\ttest-auc:0.81416\ttest-logloss:0.52702\n",
      "[150]\ttrain-error:0.17519\ttrain-auc:0.92696\ttrain-logloss:0.40652\ttest-error:0.27100\ttest-auc:0.81404\ttest-logloss:0.51967\n",
      "[200]\ttrain-error:0.16012\ttrain-auc:0.93700\ttrain-logloss:0.38730\ttest-error:0.27075\ttest-auc:0.81373\ttest-logloss:0.51851\n",
      "[232]\ttrain-error:0.15313\ttrain-auc:0.94268\ttrain-logloss:0.37822\ttest-error:0.27050\ttest-auc:0.81354\ttest-logloss:0.51848\n",
      "Stopping after 234 rounds                                                         \n",
      "[0]\ttrain-error:0.21263\ttrain-auc:0.87289\ttrain-logloss:0.68168\ttest-error:0.32750\ttest-auc:0.75360\ttest-logloss:0.68579\n",
      "[50]\ttrain-error:0.05744\ttrain-auc:0.98418\ttrain-logloss:0.38848\ttest-error:0.27725\ttest-auc:0.80692\ttest-logloss:0.54460\n",
      "[100]\ttrain-error:0.02937\ttrain-auc:0.99389\ttrain-logloss:0.29105\ttest-error:0.27650\ttest-auc:0.80911\ttest-logloss:0.52489\n",
      "[150]\ttrain-error:0.01725\ttrain-auc:0.99704\ttrain-logloss:0.25156\ttest-error:0.27675\ttest-auc:0.80895\ttest-logloss:0.52446\n",
      "[179]\ttrain-error:0.01225\ttrain-auc:0.99793\ttrain-logloss:0.24195\ttest-error:0.27500\ttest-auc:0.80947\ttest-logloss:0.52473\n",
      "Stopping after 180 rounds                                                         \n",
      "[0]\ttrain-error:0.27031\ttrain-auc:0.81262\ttrain-logloss:0.67798\ttest-error:0.27300\ttest-auc:0.79846\ttest-logloss:0.67874\n",
      "[50]\ttrain-error:0.25062\ttrain-auc:0.84717\ttrain-logloss:0.48757\ttest-error:0.27650\ttest-auc:0.81196\ttest-logloss:0.52289\n",
      "[100]\ttrain-error:0.24413\ttrain-auc:0.85869\ttrain-logloss:0.46593\ttest-error:0.27350\ttest-auc:0.81286\ttest-logloss:0.51875\n",
      "[150]\ttrain-error:0.22569\ttrain-auc:0.87695\ttrain-logloss:0.44612\ttest-error:0.27750\ttest-auc:0.81161\ttest-logloss:0.52024\n",
      "[153]\ttrain-error:0.22506\ttrain-auc:0.87762\ttrain-logloss:0.44529\ttest-error:0.27700\ttest-auc:0.81164\ttest-logloss:0.52022\n",
      "Stopping after 155 rounds                                                         \n",
      "100%|██████████| 100/100 [37:59<00:00, 22.80s/trial, best loss: 0.5160313823748147]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_experiment(\"hyperopt_test_5_subset\")\n",
    "\n",
    "mlflow.xgboost.autolog(\n",
    "    log_input_examples=False, log_model_signatures=True, log_models=True\n",
    ")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"test\"):\n",
    "    best_params = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id                                            1b2aa9c22f4347e08d61d26a59074707\n",
      "experiment_id                                                                    8\n",
      "status                                                                    FINISHED\n",
      "artifact_uri                     ./mlruns/8/1b2aa9c22f4347e08d61d26a59074707/ar...\n",
      "start_time                                        2022-11-04 20:32:16.880000+00:00\n",
      "end_time                                          2022-11-04 20:32:36.246000+00:00\n",
      "metrics.test-auc                                                          0.809102\n",
      "metrics.train-error                                                         0.0215\n",
      "metrics.test-error                                                           0.277\n",
      "metrics.best_iteration                                                       129.0\n",
      "metrics.test-logloss                                                      0.523911\n",
      "metrics.train-auc                                                         0.996038\n",
      "metrics.train-logloss                                                     0.264892\n",
      "metrics.stopped_iteration                                                    179.0\n",
      "params.eval_metric                                     ['error', 'auc', 'logloss']\n",
      "params.num_boost_round                                                        1000\n",
      "params.early_stopping_rounds                                                    50\n",
      "params.objective                                                   binary:logistic\n",
      "params.subsample                                                0.7422734263179792\n",
      "params.min_child_weight                                         3.0141298521224598\n",
      "params.colsample_bytree                                         0.5162894174992337\n",
      "params.verbose_eval                                                             50\n",
      "params.maximize                                                               None\n",
      "params.custom_metric                                                          None\n",
      "params.seed                                                                     41\n",
      "params.gamma                                                   0.09123272667907334\n",
      "params.eta                                                     0.02631154672937419\n",
      "params.max_depth                                                                16\n",
      "params.lambda                                                   0.8360839040798562\n",
      "params.alpha                                                  0.036698410820945715\n",
      "params.colsample_bylevel                                         0.406090894491499\n",
      "tags.mlflow.log-model.history    [{\"run_id\": \"1b2aa9c22f4347e08d61d26a59074707\"...\n",
      "tags.mlflow.source.type                                                      LOCAL\n",
      "tags.mlflow.user                                                            m98612\n",
      "tags.mlflow.runName                                                popular-yak-202\n",
      "tags.mlflow.parentRunId                           7a3f125cf0eb47c5b13dd1ffc9484b46\n",
      "tags.mlflow.source.name          /Users/m98612/Library/Python/3.10/lib/python/s...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#current best model:\n",
    "# Load model as a PyFuncModel.\n",
    "#path to local model:\n",
    "logged_model2 = \"mlruns/7/83515d1538024eaa852eb684ebaa1c02/artifacts/model\"\n",
    "\n",
    "#loaded_model1 = mlflow.xgboost.load_model(logged_model1)\n",
    "loaded_model2 = mlflow.xgboost.load_model(logged_model2)\n",
    "loaded_model3 = mlflow.pyfunc.load_model(logged_model2)\n",
    "# find the best model from the run:\n",
    "best_run = mlflow.search_runs(experiment_ids=mlflow.get_experiment_by_name(\"hyperopt_test_5_subset\").experiment_id, order_by=[\"metrics.test_logloss ASC\"]).iloc[1]\n",
    "\n",
    "print(best_run)\n",
    "\n",
    "# load the best model:\n",
    "loaded_model4 = mlflow.xgboost.load_model(f\"runs:/{best_run.run_id}/model\")\n",
    "loaded_model4 = mlflow.pyfunc.load_model(f\"runs:/{best_run.run_id}/model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xgboost.core.Booster object at 0x1753920b0>\n",
      "mlflow.pyfunc.loaded_model:\n",
      "  artifact_path: model\n",
      "  flavor: mlflow.xgboost\n",
      "  run_id: cd1d6d5f985044c58327ff5a92d0510d\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model2)\n",
    "print(loaded_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Validate model on the validation_df_500 \n",
    "\n",
    "validation_df_500 = validation_df[list_of_K_best_features]\n",
    "#Prep the submission format:\n",
    "validation_df_500[\"pred\"]= loaded_model3.predict(validation_df_500)\n",
    "validation_df_500[\"id\"] = validation_df.index\n",
    "\n",
    "#save to file:\n",
    "validation_df_500[[\"id\", \"pred\"]].to_csv(\"results/submission_hyperopt_best500_preprocess3.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Try Tabnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_tabnet\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer    \n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#-----------------------------------------------\n",
    "# callback object for MLflow integration\n",
    "#-----------------------------------------------\n",
    "class MLCallback(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \n",
    "        mlflow.set_tracking_uri(MLF_TRACK_URI)\n",
    "        mlflow.set_experiment(MLF_EXP_NAME)\n",
    "        mlflow.start_run()\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        \n",
    "        #log model:\n",
    "        #save model to mlflow:\n",
    "        #save model temporary local as pickle:\n",
    "        \n",
    "        with open(\"model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(clf, f)\n",
    "\n",
    "        #log the artifact to local folder:\n",
    "        mlflow.log_artifact(\"model.pkl\")\n",
    "\n",
    "\n",
    "        #remove the local file:\n",
    "        os.remove(\"model.pkl\")\n",
    "        \n",
    "\n",
    "        #log model params:\n",
    "        mlflow.end_run()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "    \n",
    "        # send to MLFlow\n",
    "        mlflow.log_metric(\"train_auc\", logs['train_auc'])\n",
    "        mlflow.log_metric(\"test_auc\", logs[\"test_auc\"])\n",
    "        # log logloss\n",
    "        mlflow.log_metric(\"train_logloss\", logs['train_logloss'])\n",
    "        # log test logloss\n",
    "        mlflow.log_metric(\"test_logloss\", logs[\"test_logloss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tabnet Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22157633, 0.28326884, 0.36699717, ..., 0.4287035 , 0.4467494 ,\n",
       "        0.51951046],\n",
       "       [0.39118822, 0.25936471, 0.53240802, ..., 0.60263593, 0.66646506,\n",
       "        0.51928646],\n",
       "       [0.19429985, 0.36670328, 0.38280369, ..., 0.43693889, 0.51667208,\n",
       "        0.43606572],\n",
       "       ...,\n",
       "       [0.07192755, 0.33848796, 0.47368865, ..., 0.466203  , 0.46950663,\n",
       "        0.42194336],\n",
       "       [0.28597205, 0.31722673, 0.50084435, ..., 0.46477771, 0.45796893,\n",
       "        0.53304558],\n",
       "       [0.01855163, 0.36866017, 0.47478804, ..., 0.44457589, 0.48429303,\n",
       "        0.5243973 ]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing of the PCA dataframe:\n",
    "#-----------------------------------------------\n",
    "#drop label: \n",
    "\n",
    "# scale the data between 0 and 1:\n",
    "scaler = MinMaxScaler()\n",
    "traintest_df_PCA = scaler.fit_transform(traintest_df_PCA)\n",
    "\n",
    "\n",
    "#use the scaler on validation_df_PCA:\n",
    "validation_df_PCA = scaler.transform(validation_df_PCA)\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "#add label to the PCA dataframe:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 800)\n",
      "(4000, 800)\n",
      "(16000,)\n",
      "(4000,)\n"
     ]
    }
   ],
   "source": [
    "X, y  = traintest_df_PCA.drop(\"label\", axis=1), traintest_df_PCA[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#-----------------------------------------------#print all shapes:\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tabnet Model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70023 | train_auc: 0.6541  | train_logloss: 0.71857 | test_auc: 0.6582  | test_logloss: 0.72488 |  0:00:05s\n",
      "epoch 1  | loss: 0.67619 | train_auc: 0.5717  | train_logloss: 0.71469 | test_auc: 0.56562 | test_logloss: 0.72067 |  0:00:10s\n",
      "epoch 2  | loss: 0.67342 | train_auc: 0.57174 | train_logloss: 0.72079 | test_auc: 0.5754  | test_logloss: 0.72771 |  0:00:15s\n",
      "epoch 3  | loss: 0.67178 | train_auc: 0.61435 | train_logloss: 0.73207 | test_auc: 0.6108  | test_logloss: 0.74158 |  0:00:20s\n",
      "epoch 4  | loss: 0.66358 | train_auc: 0.6175  | train_logloss: 0.73487 | test_auc: 0.615   | test_logloss: 0.74684 |  0:00:24s\n",
      "epoch 5  | loss: 0.66801 | train_auc: 0.62672 | train_logloss: 0.71838 | test_auc: 0.62954 | test_logloss: 0.72613 |  0:00:29s\n",
      "epoch 6  | loss: 0.66488 | train_auc: 0.65482 | train_logloss: 0.73229 | test_auc: 0.65844 | test_logloss: 0.7418  |  0:00:33s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [185], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m clf \u001b[39m=\u001b[39m TabNetClassifier()\n\u001b[1;32m     16\u001b[0m \u001b[39m#eval set: ['auc', 'accuracy', 'balanced_accuracy', 'logloss', 'mae', 'mse', 'rmsle', 'unsup_loss', 'unsup_loss_numpy', 'rmse']\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m clf\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     18\u001b[0m     X_train\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m=\u001b[39;49my_train\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     19\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(X_train\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues), (X_test\u001b[39m.\u001b[39;49mvalues, y_test\u001b[39m.\u001b[39;49mvalues)],\n\u001b[1;32m     20\u001b[0m     eval_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     21\u001b[0m     eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mauc\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mlogloss\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     22\u001b[0m     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m528\u001b[39;49m, virtual_batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m     weights\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[mlcbck],\n\u001b[1;32m     27\u001b[0m     drop_last\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:245\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_epoch(eval_name, valid_dataloader)\n\u001b[1;32m    247\u001b[0m \u001b[39m# Call method on_epoch_end for all callbacks\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_end(\n\u001b[1;32m    249\u001b[0m     epoch_idx, logs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mepoch_metrics\n\u001b[1;32m    250\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:530\u001b[0m, in \u001b[0;36mTabModel._predict_epoch\u001b[0;34m(self, name, loader)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m# Main loop\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m--> 530\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_batch(X)\n\u001b[1;32m    531\u001b[0m     list_y_true\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m    532\u001b[0m     list_y_score\u001b[39m.\u001b[39mappend(scores)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:558\u001b[0m, in \u001b[0;36mTabModel._predict_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    555\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    557\u001b[0m \u001b[39m# compute model output\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m scores, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scores, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    561\u001b[0m     scores \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scores]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[0;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:471\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    470\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 471\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    472\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[1;32m    475\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:168\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m# output\u001b[39;00m\n\u001b[1;32m    167\u001b[0m masked_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(M, x)\n\u001b[0;32m--> 168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeat_transformers[step](masked_x)\n\u001b[1;32m    169\u001b[0m d \u001b[39m=\u001b[39m ReLU()(out[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_d])\n\u001b[1;32m    170\u001b[0m steps_output\u001b[39m.\u001b[39mappend(d)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:706\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 706\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared(x)\n\u001b[1;32m    707\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecifics(x)\n\u001b[1;32m    708\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:743\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    741\u001b[0m scale \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mFloatTensor([\u001b[39m0.5\u001b[39m])\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    742\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst:  \u001b[39m# the first layer of the block has no scale multiplication\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglu_layers[\u001b[39m0\u001b[39;49m](x)\n\u001b[1;32m    744\u001b[0m     layers_left \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_glu)\n\u001b[1;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:774\u001b[0m, in \u001b[0;36mGLU_Layer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    772\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m    773\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[0;32m--> 774\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim], torch\u001b[39m.\u001b[39;49msigmoid(x[:, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_dim :]))\n\u001b[1;32m    775\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run tabnet:\n",
    "#define constants:\n",
    "\n",
    "mlflow.end_run()\n",
    "MLF_TRACK_URI = \"http://localhost:5000\"\n",
    "MLF_EXP_NAME = \"tabnet_runs\"\n",
    "mlcbck = MLCallback()\n",
    "\n",
    "#------------------EXPLANATION-----------------------------\n",
    "#batch size means how many rows are used for each gradient update\n",
    "#virtual batch size on the other hand is the number of rows used for each gradient update before the weights are updated\n",
    "#num workers is the number of threads used for data loading\n",
    "#weights is the weight of each class and should be used if the classes are unbalanced\n",
    "#----------------------------------------------------------\n",
    "clf = TabNetClassifier()\n",
    "#eval set: ['auc', 'accuracy', 'balanced_accuracy', 'logloss', 'mae', 'mse', 'rmsle', 'unsup_loss', 'unsup_loss_numpy', 'rmse']\n",
    "clf.fit(\n",
    "    X_train=X_train.values, y_train=y_train.values,\n",
    "    eval_set=[(X_train.values, y_train.values), (X_test.values, y_test.values)],\n",
    "    eval_name=['train', 'test'],\n",
    "    eval_metric=['auc','logloss'],\n",
    "    max_epochs=1000, patience=50,\n",
    "    batch_size=528, virtual_batch_size=64,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    callbacks=[mlcbck],\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.63958 | train_auc: 0.26199 | train_logloss: 1.85492 | test_auc: 0.26135 | test_logloss: 1.81979 |  0:00:04s\n",
      "epoch 1  | loss: 2.63943 | train_auc: 0.26246 | train_logloss: 1.64784 | test_auc: 0.2612  | test_logloss: 1.61769 |  0:00:08s\n",
      "epoch 2  | loss: 2.62139 | train_auc: 0.2918  | train_logloss: 1.50202 | test_auc: 0.28578 | test_logloss: 1.47743 |  0:00:11s\n",
      "epoch 3  | loss: 2.55981 | train_auc: 0.32408 | train_logloss: 1.45101 | test_auc: 0.31266 | test_logloss: 1.44091 |  0:00:13s\n",
      "epoch 4  | loss: 2.57852 | train_auc: 0.35862 | train_logloss: 1.38771 | test_auc: 0.35603 | test_logloss: 1.36949 |  0:00:16s\n",
      "epoch 5  | loss: 2.58327 | train_auc: 0.38609 | train_logloss: 1.34734 | test_auc: 0.38508 | test_logloss: 1.33214 |  0:00:19s\n",
      "epoch 6  | loss: 2.57601 | train_auc: 0.40189 | train_logloss: 1.48223 | test_auc: 0.40282 | test_logloss: 1.46611 |  0:00:21s\n",
      "epoch 7  | loss: 2.57149 | train_auc: 0.42288 | train_logloss: 1.47964 | test_auc: 0.41702 | test_logloss: 1.47014 |  0:00:24s\n",
      "epoch 8  | loss: 2.48204 | train_auc: 0.43828 | train_logloss: 1.57133 | test_auc: 0.42831 | test_logloss: 1.56023 |  0:00:27s\n",
      "epoch 9  | loss: 2.50518 | train_auc: 0.45632 | train_logloss: 1.63779 | test_auc: 0.44192 | test_logloss: 1.6147  |  0:00:29s\n",
      "epoch 10 | loss: 2.54565 | train_auc: 0.46635 | train_logloss: 1.6857  | test_auc: 0.45109 | test_logloss: 1.70227 |  0:00:32s\n",
      "epoch 11 | loss: 2.4432  | train_auc: 0.46757 | train_logloss: 1.79799 | test_auc: 0.45399 | test_logloss: 1.78873 |  0:00:35s\n",
      "epoch 12 | loss: 2.47873 | train_auc: 0.47632 | train_logloss: 1.83388 | test_auc: 0.46261 | test_logloss: 1.84523 |  0:00:37s\n",
      "epoch 13 | loss: 2.40525 | train_auc: 0.47963 | train_logloss: 1.91755 | test_auc: 0.46198 | test_logloss: 1.93399 |  0:00:40s\n",
      "epoch 14 | loss: 2.43422 | train_auc: 0.483   | train_logloss: 1.92108 | test_auc: 0.46776 | test_logloss: 1.91239 |  0:00:42s\n",
      "epoch 15 | loss: 2.4202  | train_auc: 0.48531 | train_logloss: 1.99188 | test_auc: 0.47049 | test_logloss: 2.00299 |  0:00:45s\n",
      "epoch 16 | loss: 2.4214  | train_auc: 0.48709 | train_logloss: 2.00615 | test_auc: 0.4768  | test_logloss: 1.98353 |  0:00:48s\n",
      "epoch 17 | loss: 2.37593 | train_auc: 0.48535 | train_logloss: 2.0018  | test_auc: 0.46647 | test_logloss: 2.0076  |  0:00:50s\n",
      "epoch 18 | loss: 2.34463 | train_auc: 0.48936 | train_logloss: 1.99901 | test_auc: 0.47289 | test_logloss: 2.00868 |  0:00:53s\n",
      "epoch 19 | loss: 2.37774 | train_auc: 0.49209 | train_logloss: 2.0221  | test_auc: 0.47423 | test_logloss: 2.01123 |  0:00:56s\n",
      "epoch 20 | loss: 2.32924 | train_auc: 0.49511 | train_logloss: 1.95752 | test_auc: 0.47627 | test_logloss: 1.96592 |  0:00:58s\n",
      "epoch 21 | loss: 2.35091 | train_auc: 0.49048 | train_logloss: 1.98887 | test_auc: 0.47929 | test_logloss: 1.98113 |  0:01:01s\n",
      "epoch 22 | loss: 2.27737 | train_auc: 0.49387 | train_logloss: 1.95574 | test_auc: 0.48073 | test_logloss: 1.93749 |  0:01:03s\n",
      "epoch 23 | loss: 2.32511 | train_auc: 0.49241 | train_logloss: 1.96184 | test_auc: 0.47861 | test_logloss: 1.96242 |  0:01:06s\n",
      "epoch 24 | loss: 2.26171 | train_auc: 0.49415 | train_logloss: 1.92625 | test_auc: 0.48322 | test_logloss: 1.91575 |  0:01:09s\n",
      "epoch 25 | loss: 2.30592 | train_auc: 0.49326 | train_logloss: 2.02145 | test_auc: 0.47904 | test_logloss: 2.00596 |  0:01:12s\n",
      "epoch 26 | loss: 2.27879 | train_auc: 0.4993  | train_logloss: 1.94757 | test_auc: 0.4828  | test_logloss: 1.94222 |  0:01:14s\n",
      "epoch 27 | loss: 2.22389 | train_auc: 0.49663 | train_logloss: 1.96624 | test_auc: 0.48077 | test_logloss: 1.94523 |  0:01:17s\n",
      "epoch 28 | loss: 2.2005  | train_auc: 0.49762 | train_logloss: 1.94076 | test_auc: 0.48625 | test_logloss: 1.94126 |  0:01:19s\n",
      "epoch 29 | loss: 2.22846 | train_auc: 0.49813 | train_logloss: 1.9345  | test_auc: 0.48493 | test_logloss: 1.91398 |  0:01:22s\n",
      "epoch 30 | loss: 2.18994 | train_auc: 0.49829 | train_logloss: 1.90461 | test_auc: 0.48775 | test_logloss: 1.88236 |  0:01:25s\n",
      "                                                       \n",
      "Early stopping occurred at epoch 30 with best_epoch = 5 and best_test_logloss = 1.33214\n",
      "epoch 0  | loss: 1.58189 | train_auc: 0.7558  | train_logloss: 0.73122 | test_auc: 0.74991 | test_logloss: 0.74582 |  0:00:15s\n",
      "epoch 1  | loss: 1.62577 | train_auc: 0.76685 | train_logloss: 0.69802 | test_auc: 0.76835 | test_logloss: 0.70772 |  0:00:27s\n",
      "epoch 2  | loss: 1.59386 | train_auc: 0.77058 | train_logloss: 0.6778  | test_auc: 0.77528 | test_logloss: 0.68592 |  0:00:39s\n",
      "epoch 3  | loss: 1.61415 | train_auc: 0.77016 | train_logloss: 0.66164 | test_auc: 0.7762  | test_logloss: 0.66764 |  0:00:51s\n",
      "epoch 4  | loss: 1.57391 | train_auc: 0.77453 | train_logloss: 0.63923 | test_auc: 0.77949 | test_logloss: 0.64528 |  0:01:07s\n",
      "epoch 5  | loss: 1.52572 | train_auc: 0.73485 | train_logloss: 0.66426 | test_auc: 0.73749 | test_logloss: 0.67514 |  0:01:21s\n",
      "epoch 6  | loss: 1.54416 | train_auc: 0.73785 | train_logloss: 0.6584  | test_auc: 0.7379  | test_logloss: 0.66908 |  0:01:35s\n",
      "epoch 7  | loss: 1.59314 | train_auc: 0.74279 | train_logloss: 0.65825 | test_auc: 0.74414 | test_logloss: 0.69453 |  0:01:52s\n",
      "epoch 8  | loss: 1.55853 | train_auc: 0.71985 | train_logloss: 0.71885 | test_auc: 0.73096 | test_logloss: 0.70388 |  0:02:08s\n",
      "epoch 9  | loss: 1.54924 | train_auc: 0.69547 | train_logloss: 0.79832 | test_auc: 0.69318 | test_logloss: 0.84592 |  0:02:21s\n",
      "epoch 10 | loss: 1.52922 | train_auc: 0.6809  | train_logloss: 0.86512 | test_auc: 0.67392 | test_logloss: 0.88659 |  0:02:35s\n",
      "epoch 11 | loss: 1.53622 | train_auc: 0.67346 | train_logloss: 0.91808 | test_auc: 0.6671  | test_logloss: 0.93817 |  0:02:49s\n",
      "epoch 12 | loss: 1.53299 | train_auc: 0.64114 | train_logloss: 0.96677 | test_auc: 0.64282 | test_logloss: 0.96693 |  0:03:02s\n",
      "epoch 13 | loss: 1.5423  | train_auc: 0.6516  | train_logloss: 0.9844  | test_auc: 0.65579 | test_logloss: 0.98015 |  0:03:15s\n",
      "epoch 14 | loss: 1.55072 | train_auc: 0.63328 | train_logloss: 1.01577 | test_auc: 0.63859 | test_logloss: 1.03214 |  0:03:27s\n",
      "epoch 15 | loss: 1.53609 | train_auc: 0.63457 | train_logloss: 1.02543 | test_auc: 0.63866 | test_logloss: 1.04851 |  0:03:40s\n",
      "epoch 16 | loss: 1.51742 | train_auc: 0.64288 | train_logloss: 1.04448 | test_auc: 0.6409  | test_logloss: 1.09383 |  0:03:52s\n",
      "epoch 17 | loss: 1.54642 | train_auc: 0.63484 | train_logloss: 1.07204 | test_auc: 0.63473 | test_logloss: 1.06814 |  0:04:05s\n",
      "epoch 18 | loss: 1.50633 | train_auc: 0.62069 | train_logloss: 1.08425 | test_auc: 0.62582 | test_logloss: 1.09015 |  0:04:18s\n",
      "epoch 19 | loss: 1.55934 | train_auc: 0.64482 | train_logloss: 1.02866 | test_auc: 0.65149 | test_logloss: 1.00464 |  0:04:31s\n",
      "epoch 20 | loss: 1.49175 | train_auc: 0.64859 | train_logloss: 1.11861 | test_auc: 0.65195 | test_logloss: 1.13782 |  0:04:43s\n",
      "epoch 21 | loss: 1.49089 | train_auc: 0.6346  | train_logloss: 1.15188 | test_auc: 0.65278 | test_logloss: 1.12324 |  0:04:56s\n",
      "epoch 22 | loss: 1.56352 | train_auc: 0.65947 | train_logloss: 1.06727 | test_auc: 0.66296 | test_logloss: 1.07616 |  0:05:10s\n",
      "epoch 23 | loss: 1.5183  | train_auc: 0.65577 | train_logloss: 1.04654 | test_auc: 0.65129 | test_logloss: 1.09762 |  0:05:23s\n",
      "epoch 24 | loss: 1.50842 | train_auc: 0.64039 | train_logloss: 1.12608 | test_auc: 0.64377 | test_logloss: 1.07558 |  0:05:35s\n",
      "epoch 25 | loss: 1.50066 | train_auc: 0.63819 | train_logloss: 1.11604 | test_auc: 0.64449 | test_logloss: 1.04736 |  0:05:47s\n",
      "epoch 26 | loss: 1.50476 | train_auc: 0.63787 | train_logloss: 1.15389 | test_auc: 0.65881 | test_logloss: 1.17658 |  0:06:00s\n",
      "epoch 27 | loss: 1.52778 | train_auc: 0.64207 | train_logloss: 1.11986 | test_auc: 0.6392  | test_logloss: 1.18145 |  0:06:14s\n",
      "epoch 28 | loss: 1.49891 | train_auc: 0.62876 | train_logloss: 1.10262 | test_auc: 0.62974 | test_logloss: 1.07164 |  0:06:30s\n",
      "epoch 29 | loss: 1.54909 | train_auc: 0.62982 | train_logloss: 1.11087 | test_auc: 0.63267 | test_logloss: 1.12193 |  0:06:44s\n",
      "                                                                                  \n",
      "Early stopping occurred at epoch 29 with best_epoch = 4 and best_test_logloss = 0.64528\n",
      "epoch 0  | loss: 1.5622  | train_auc: 0.54667 | train_logloss: 0.80448 | test_auc: 0.55221 | test_logloss: 0.79495 |  0:00:14s\n",
      "epoch 1  | loss: 1.45145 | train_auc: 0.45679 | train_logloss: 0.93358 | test_auc: 0.46817 | test_logloss: 0.92044 |  0:00:26s\n",
      "epoch 2  | loss: 1.36836 | train_auc: 0.4679  | train_logloss: 0.85543 | test_auc: 0.46099 | test_logloss: 0.84874 |  0:00:37s\n",
      "epoch 3  | loss: 1.36465 | train_auc: 0.45489 | train_logloss: 0.83429 | test_auc: 0.45311 | test_logloss: 0.82915 |  0:00:50s\n",
      "epoch 4  | loss: 1.325   | train_auc: 0.4755  | train_logloss: 0.79581 | test_auc: 0.48381 | test_logloss: 0.78629 |  0:01:01s\n",
      "epoch 5  | loss: 1.29498 | train_auc: 0.52877 | train_logloss: 0.76297 | test_auc: 0.53609 | test_logloss: 0.74623 |  0:01:13s\n",
      "epoch 6  | loss: 1.24056 | train_auc: 0.4646  | train_logloss: 0.86749 | test_auc: 0.45173 | test_logloss: 0.87894 |  0:01:26s\n",
      "epoch 7  | loss: 1.22287 | train_auc: 0.53151 | train_logloss: 0.80662 | test_auc: 0.52863 | test_logloss: 0.7963  |  0:01:39s\n",
      "epoch 8  | loss: 1.22939 | train_auc: 0.47093 | train_logloss: 0.92238 | test_auc: 0.46325 | test_logloss: 0.93534 |  0:01:51s\n",
      "epoch 9  | loss: 1.09121 | train_auc: 0.55417 | train_logloss: 0.8209  | test_auc: 0.54094 | test_logloss: 0.83507 |  0:02:02s\n",
      "epoch 10 | loss: 1.05049 | train_auc: 0.57345 | train_logloss: 0.81036 | test_auc: 0.56059 | test_logloss: 0.8466  |  0:02:16s\n",
      "epoch 11 | loss: 1.00492 | train_auc: 0.56248 | train_logloss: 0.82428 | test_auc: 0.55309 | test_logloss: 0.83857 |  0:02:30s\n",
      "epoch 12 | loss: 0.97427 | train_auc: 0.61096 | train_logloss: 0.73434 | test_auc: 0.61708 | test_logloss: 0.71475 |  0:02:42s\n",
      "epoch 13 | loss: 0.91391 | train_auc: 0.54793 | train_logloss: 0.80407 | test_auc: 0.53407 | test_logloss: 0.80432 |  0:02:55s\n",
      "epoch 14 | loss: 0.84158 | train_auc: 0.68556 | train_logloss: 0.66218 | test_auc: 0.68574 | test_logloss: 0.65949 |  0:03:06s\n",
      "epoch 15 | loss: 0.8149  | train_auc: 0.60916 | train_logloss: 0.80741 | test_auc: 0.59844 | test_logloss: 0.83374 |  0:03:18s\n",
      "epoch 16 | loss: 0.79931 | train_auc: 0.64598 | train_logloss: 0.70671 | test_auc: 0.62639 | test_logloss: 0.73485 |  0:03:30s\n",
      "epoch 17 | loss: 0.76922 | train_auc: 0.65967 | train_logloss: 0.67662 | test_auc: 0.65694 | test_logloss: 0.67567 |  0:03:41s\n",
      "epoch 18 | loss: 0.73665 | train_auc: 0.70132 | train_logloss: 0.64441 | test_auc: 0.69713 | test_logloss: 0.64767 |  0:03:53s\n",
      "epoch 19 | loss: 0.7171  | train_auc: 0.63633 | train_logloss: 0.69924 | test_auc: 0.64104 | test_logloss: 0.68882 |  0:04:04s\n",
      "epoch 20 | loss: 0.69417 | train_auc: 0.73643 | train_logloss: 0.63344 | test_auc: 0.73633 | test_logloss: 0.6383  |  0:04:15s\n",
      "epoch 21 | loss: 0.67972 | train_auc: 0.74857 | train_logloss: 0.5888  | test_auc: 0.75192 | test_logloss: 0.59123 |  0:04:27s\n",
      "epoch 22 | loss: 0.67525 | train_auc: 0.65095 | train_logloss: 0.66514 | test_auc: 0.65091 | test_logloss: 0.66133 |  0:04:38s\n",
      "epoch 23 | loss: 0.69634 | train_auc: 0.69929 | train_logloss: 0.71389 | test_auc: 0.71111 | test_logloss: 0.71057 |  0:04:50s\n",
      "epoch 24 | loss: 0.66849 | train_auc: 0.75198 | train_logloss: 0.59602 | test_auc: 0.76109 | test_logloss: 0.58689 |  0:05:01s\n",
      "epoch 25 | loss: 0.64228 | train_auc: 0.76775 | train_logloss: 0.5773  | test_auc: 0.77139 | test_logloss: 0.57337 |  0:05:13s\n",
      "epoch 26 | loss: 0.6369  | train_auc: 0.76263 | train_logloss: 0.58751 | test_auc: 0.7698  | test_logloss: 0.58392 |  0:05:25s\n",
      "epoch 27 | loss: 0.63773 | train_auc: 0.75642 | train_logloss: 0.5986  | test_auc: 0.76666 | test_logloss: 0.58812 |  0:05:36s\n",
      "epoch 28 | loss: 0.65466 | train_auc: 0.74297 | train_logloss: 0.61224 | test_auc: 0.75554 | test_logloss: 0.60916 |  0:05:47s\n",
      "epoch 29 | loss: 0.65971 | train_auc: 0.71778 | train_logloss: 0.61467 | test_auc: 0.7245  | test_logloss: 0.60935 |  0:05:59s\n",
      "epoch 30 | loss: 0.64419 | train_auc: 0.74493 | train_logloss: 0.60374 | test_auc: 0.75352 | test_logloss: 0.60323 |  0:06:12s\n",
      "epoch 31 | loss: 0.64009 | train_auc: 0.71834 | train_logloss: 0.62277 | test_auc: 0.72091 | test_logloss: 0.61975 |  0:06:23s\n",
      "epoch 32 | loss: 0.63508 | train_auc: 0.77983 | train_logloss: 0.56686 | test_auc: 0.78326 | test_logloss: 0.56756 |  0:06:34s\n",
      "epoch 33 | loss: 0.62135 | train_auc: 0.77335 | train_logloss: 0.57073 | test_auc: 0.77396 | test_logloss: 0.57074 |  0:06:45s\n",
      "epoch 34 | loss: 0.62682 | train_auc: 0.76859 | train_logloss: 0.58461 | test_auc: 0.77939 | test_logloss: 0.5808  |  0:06:56s\n",
      "epoch 35 | loss: 0.72224 | train_auc: 0.74045 | train_logloss: 0.59886 | test_auc: 0.74742 | test_logloss: 0.59584 |  0:07:07s\n",
      "epoch 36 | loss: 0.63261 | train_auc: 0.76564 | train_logloss: 0.59629 | test_auc: 0.76705 | test_logloss: 0.59372 |  0:07:18s\n",
      "epoch 37 | loss: 0.63164 | train_auc: 0.7549  | train_logloss: 0.60979 | test_auc: 0.75926 | test_logloss: 0.60932 |  0:07:28s\n",
      "epoch 38 | loss: 0.62758 | train_auc: 0.78602 | train_logloss: 0.56785 | test_auc: 0.78653 | test_logloss: 0.5645  |  0:07:39s\n",
      "epoch 39 | loss: 0.61041 | train_auc: 0.75671 | train_logloss: 0.58699 | test_auc: 0.76021 | test_logloss: 0.58613 |  0:07:50s\n",
      "epoch 40 | loss: 0.62496 | train_auc: 0.76201 | train_logloss: 0.58959 | test_auc: 0.77195 | test_logloss: 0.58504 |  0:08:00s\n",
      "epoch 41 | loss: 0.61533 | train_auc: 0.7693  | train_logloss: 0.58529 | test_auc: 0.7763  | test_logloss: 0.58297 |  0:08:11s\n",
      "epoch 42 | loss: 0.61114 | train_auc: 0.76009 | train_logloss: 0.58886 | test_auc: 0.76362 | test_logloss: 0.58323 |  0:08:23s\n",
      "epoch 43 | loss: 0.61015 | train_auc: 0.77693 | train_logloss: 0.57251 | test_auc: 0.78669 | test_logloss: 0.56574 |  0:08:33s\n",
      "epoch 44 | loss: 0.61006 | train_auc: 0.78379 | train_logloss: 0.563   | test_auc: 0.79712 | test_logloss: 0.55274 |  0:08:44s\n",
      "epoch 45 | loss: 0.59796 | train_auc: 0.7722  | train_logloss: 0.57615 | test_auc: 0.78633 | test_logloss: 0.56036 |  0:08:57s\n",
      "epoch 46 | loss: 0.58621 | train_auc: 0.79031 | train_logloss: 0.55108 | test_auc: 0.79772 | test_logloss: 0.54693 |  0:09:08s\n",
      "epoch 47 | loss: 0.58761 | train_auc: 0.79572 | train_logloss: 0.54992 | test_auc: 0.80038 | test_logloss: 0.545   |  0:09:26s\n",
      "epoch 48 | loss: 0.59256 | train_auc: 0.79348 | train_logloss: 0.54945 | test_auc: 0.80162 | test_logloss: 0.54306 |  0:09:40s\n",
      "epoch 49 | loss: 0.58532 | train_auc: 0.7955  | train_logloss: 0.54646 | test_auc: 0.79897 | test_logloss: 0.54296 |  0:09:52s\n",
      "epoch 50 | loss: 0.60117 | train_auc: 0.76322 | train_logloss: 0.60588 | test_auc: 0.77099 | test_logloss: 0.60488 |  0:10:02s\n",
      "epoch 51 | loss: 0.59846 | train_auc: 0.79372 | train_logloss: 0.55063 | test_auc: 0.80171 | test_logloss: 0.54417 |  0:10:13s\n",
      "epoch 52 | loss: 0.59581 | train_auc: 0.75711 | train_logloss: 0.61708 | test_auc: 0.76734 | test_logloss: 0.61579 |  0:10:24s\n",
      "epoch 53 | loss: 0.60395 | train_auc: 0.7965  | train_logloss: 0.55027 | test_auc: 0.80541 | test_logloss: 0.54093 |  0:10:34s\n",
      "epoch 54 | loss: 0.58007 | train_auc: 0.79702 | train_logloss: 0.54447 | test_auc: 0.80567 | test_logloss: 0.53899 |  0:10:45s\n",
      "epoch 55 | loss: 0.58156 | train_auc: 0.7987  | train_logloss: 0.56246 | test_auc: 0.80322 | test_logloss: 0.55323 |  0:10:55s\n",
      "epoch 56 | loss: 0.5926  | train_auc: 0.78587 | train_logloss: 0.56224 | test_auc: 0.78901 | test_logloss: 0.56124 |  0:11:06s\n",
      "epoch 57 | loss: 0.60642 | train_auc: 0.79572 | train_logloss: 0.54762 | test_auc: 0.80221 | test_logloss: 0.54243 |  0:11:16s\n",
      "epoch 58 | loss: 0.62529 | train_auc: 0.75803 | train_logloss: 0.5942  | test_auc: 0.76619 | test_logloss: 0.59075 |  0:11:27s\n",
      "epoch 59 | loss: 0.58284 | train_auc: 0.79442 | train_logloss: 0.55079 | test_auc: 0.80239 | test_logloss: 0.54188 |  0:11:37s\n",
      "epoch 60 | loss: 0.58577 | train_auc: 0.79529 | train_logloss: 0.54685 | test_auc: 0.80442 | test_logloss: 0.54145 |  0:11:48s\n",
      "epoch 61 | loss: 0.57623 | train_auc: 0.79627 | train_logloss: 0.54809 | test_auc: 0.80368 | test_logloss: 0.54251 |  0:11:58s\n",
      "epoch 62 | loss: 0.58263 | train_auc: 0.79272 | train_logloss: 0.55832 | test_auc: 0.79999 | test_logloss: 0.55425 |  0:12:08s\n",
      "epoch 63 | loss: 0.58915 | train_auc: 0.78707 | train_logloss: 0.55932 | test_auc: 0.79718 | test_logloss: 0.55156 |  0:12:19s\n",
      "epoch 64 | loss: 0.59153 | train_auc: 0.79265 | train_logloss: 0.54645 | test_auc: 0.80218 | test_logloss: 0.54012 |  0:12:29s\n",
      "epoch 65 | loss: 0.58116 | train_auc: 0.79636 | train_logloss: 0.54396 | test_auc: 0.80167 | test_logloss: 0.54045 |  0:12:39s\n",
      "epoch 66 | loss: 0.58386 | train_auc: 0.7965  | train_logloss: 0.54427 | test_auc: 0.80403 | test_logloss: 0.53679 |  0:12:50s\n",
      "epoch 67 | loss: 0.57739 | train_auc: 0.79602 | train_logloss: 0.54474 | test_auc: 0.80457 | test_logloss: 0.53699 |  0:13:00s\n",
      "epoch 68 | loss: 0.58143 | train_auc: 0.79508 | train_logloss: 0.54556 | test_auc: 0.80218 | test_logloss: 0.53894 |  0:13:11s\n",
      "epoch 69 | loss: 0.57879 | train_auc: 0.79666 | train_logloss: 0.55505 | test_auc: 0.80362 | test_logloss: 0.54743 |  0:13:21s\n",
      "epoch 70 | loss: 0.61875 | train_auc: 0.792   | train_logloss: 0.5509  | test_auc: 0.79882 | test_logloss: 0.54552 |  0:13:31s\n",
      "epoch 71 | loss: 0.58864 | train_auc: 0.79798 | train_logloss: 0.54101 | test_auc: 0.80664 | test_logloss: 0.53174 |  0:13:42s\n",
      "epoch 72 | loss: 0.57569 | train_auc: 0.79816 | train_logloss: 0.54007 | test_auc: 0.80368 | test_logloss: 0.53625 |  0:13:52s\n",
      "epoch 73 | loss: 0.57066 | train_auc: 0.79727 | train_logloss: 0.54448 | test_auc: 0.80354 | test_logloss: 0.54031 |  0:14:02s\n",
      "epoch 74 | loss: 0.58532 | train_auc: 0.78942 | train_logloss: 0.55564 | test_auc: 0.79784 | test_logloss: 0.54528 |  0:14:13s\n",
      "epoch 75 | loss: 0.58113 | train_auc: 0.78373 | train_logloss: 0.56268 | test_auc: 0.79434 | test_logloss: 0.55653 |  0:14:23s\n",
      "epoch 76 | loss: 0.58132 | train_auc: 0.78045 | train_logloss: 0.56424 | test_auc: 0.79015 | test_logloss: 0.55253 |  0:14:33s\n",
      "epoch 77 | loss: 0.5727  | train_auc: 0.799   | train_logloss: 0.54296 | test_auc: 0.80496 | test_logloss: 0.53617 |  0:14:43s\n",
      "epoch 78 | loss: 0.57912 | train_auc: 0.79578 | train_logloss: 0.55236 | test_auc: 0.80358 | test_logloss: 0.54094 |  0:14:53s\n",
      "epoch 79 | loss: 0.60667 | train_auc: 0.79265 | train_logloss: 0.54825 | test_auc: 0.80083 | test_logloss: 0.54139 |  0:15:04s\n",
      "epoch 80 | loss: 0.57334 | train_auc: 0.79769 | train_logloss: 0.54933 | test_auc: 0.80374 | test_logloss: 0.54298 |  0:15:14s\n",
      "epoch 81 | loss: 0.56986 | train_auc: 0.79515 | train_logloss: 0.54595 | test_auc: 0.80035 | test_logloss: 0.54511 |  0:15:24s\n",
      "epoch 82 | loss: 0.5705  | train_auc: 0.79772 | train_logloss: 0.54949 | test_auc: 0.80323 | test_logloss: 0.54451 |  0:15:34s\n",
      "epoch 83 | loss: 0.57467 | train_auc: 0.80041 | train_logloss: 0.54257 | test_auc: 0.80546 | test_logloss: 0.54074 |  0:15:45s\n",
      "epoch 84 | loss: 0.57553 | train_auc: 0.79822 | train_logloss: 0.54325 | test_auc: 0.80395 | test_logloss: 0.53583 |  0:15:56s\n",
      "epoch 85 | loss: 0.56691 | train_auc: 0.79779 | train_logloss: 0.54322 | test_auc: 0.80511 | test_logloss: 0.53395 |  0:16:06s\n",
      "epoch 86 | loss: 0.5799  | train_auc: 0.79545 | train_logloss: 0.54809 | test_auc: 0.80043 | test_logloss: 0.54488 |  0:16:17s\n",
      "epoch 87 | loss: 0.57398 | train_auc: 0.79664 | train_logloss: 0.54166 | test_auc: 0.80241 | test_logloss: 0.53686 |  0:16:27s\n",
      "epoch 88 | loss: 0.5701  | train_auc: 0.79962 | train_logloss: 0.53954 | test_auc: 0.80636 | test_logloss: 0.53101 |  0:16:37s\n",
      "epoch 89 | loss: 0.64346 | train_auc: 0.79793 | train_logloss: 0.55819 | test_auc: 0.80203 | test_logloss: 0.54692 |  0:16:48s\n",
      "epoch 90 | loss: 0.58416 | train_auc: 0.79415 | train_logloss: 0.55544 | test_auc: 0.80262 | test_logloss: 0.54616 |  0:16:59s\n",
      "epoch 91 | loss: 0.58317 | train_auc: 0.7986  | train_logloss: 0.54478 | test_auc: 0.80528 | test_logloss: 0.53822 |  0:17:09s\n",
      "epoch 92 | loss: 0.57066 | train_auc: 0.79831 | train_logloss: 0.55143 | test_auc: 0.80489 | test_logloss: 0.54494 |  0:17:20s\n",
      "epoch 93 | loss: 0.57175 | train_auc: 0.79945 | train_logloss: 0.54143 | test_auc: 0.80597 | test_logloss: 0.53586 |  0:17:30s\n",
      "epoch 94 | loss: 0.5653  | train_auc: 0.80036 | train_logloss: 0.54044 | test_auc: 0.80456 | test_logloss: 0.53618 |  0:17:40s\n",
      "epoch 95 | loss: 0.595   | train_auc: 0.80082 | train_logloss: 0.54045 | test_auc: 0.80654 | test_logloss: 0.53543 |  0:17:51s\n",
      "epoch 96 | loss: 0.57657 | train_auc: 0.79998 | train_logloss: 0.54346 | test_auc: 0.80546 | test_logloss: 0.53883 |  0:18:02s\n",
      "epoch 97 | loss: 0.56928 | train_auc: 0.79341 | train_logloss: 0.55222 | test_auc: 0.79973 | test_logloss: 0.54964 |  0:18:12s\n",
      "epoch 98 | loss: 0.57535 | train_auc: 0.79741 | train_logloss: 0.55433 | test_auc: 0.80435 | test_logloss: 0.54993 |  0:18:24s\n",
      "epoch 99 | loss: 0.56912 | train_auc: 0.79552 | train_logloss: 0.54426 | test_auc: 0.8044  | test_logloss: 0.53719 |  0:18:35s\n",
      "epoch 100| loss: 0.57009 | train_auc: 0.79648 | train_logloss: 0.54264 | test_auc: 0.8047  | test_logloss: 0.53484 |  0:18:46s\n",
      "epoch 101| loss: 0.56426 | train_auc: 0.79522 | train_logloss: 0.54382 | test_auc: 0.80225 | test_logloss: 0.53677 |  0:18:56s\n",
      "epoch 102| loss: 0.57213 | train_auc: 0.79599 | train_logloss: 0.54773 | test_auc: 0.80324 | test_logloss: 0.53849 |  0:19:09s\n",
      "epoch 103| loss: 0.5704  | train_auc: 0.79895 | train_logloss: 0.54263 | test_auc: 0.80449 | test_logloss: 0.53707 |  0:19:22s\n",
      "epoch 104| loss: 0.56403 | train_auc: 0.79897 | train_logloss: 0.54051 | test_auc: 0.80677 | test_logloss: 0.53075 |  0:19:35s\n",
      "  2%|▏         | 2/100 [28:03<22:54:32, 841.56s/trial, best loss: 0.6452785619013012]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [157], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m log_loss(y_test, y_pred[:,\u001b[39m1\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[39m#run the optimization:\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m best \u001b[39m=\u001b[39m fmin(\n\u001b[1;32m     67\u001b[0m     fn\u001b[39m=\u001b[39;49mobjective,\n\u001b[1;32m     68\u001b[0m     space\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m     69\u001b[0m     algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[1;32m     70\u001b[0m     max_evals\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     71\u001b[0m     trials\u001b[39m=\u001b[39;49mTrials(),\n\u001b[1;32m     72\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn [157], line 49\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(params):\n\u001b[1;32m     48\u001b[0m     clf \u001b[39m=\u001b[39m TabNetClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> 49\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     50\u001b[0m         X_train\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m=\u001b[39;49my_train\u001b[39m.\u001b[39;49mvalues,\n\u001b[1;32m     51\u001b[0m         eval_set\u001b[39m=\u001b[39;49m[(X_train\u001b[39m.\u001b[39;49mvalues, y_train\u001b[39m.\u001b[39;49mvalues), (X_test\u001b[39m.\u001b[39;49mvalues, y_test\u001b[39m.\u001b[39;49mvalues)],\n\u001b[1;32m     52\u001b[0m         eval_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     53\u001b[0m         eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mauc\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mlogloss\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     54\u001b[0m         max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m,\n\u001b[1;32m     55\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m528\u001b[39;49m, virtual_batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m     56\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     57\u001b[0m         weights\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     58\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[mlcbck],\n\u001b[1;32m     59\u001b[0m         drop_last\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(X_test\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     62\u001b[0m     \u001b[39m#optimize on logloss:\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:245\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_epoch(eval_name, valid_dataloader)\n\u001b[1;32m    247\u001b[0m \u001b[39m# Call method on_epoch_end for all callbacks\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_end(\n\u001b[1;32m    249\u001b[0m     epoch_idx, logs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mepoch_metrics\n\u001b[1;32m    250\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:530\u001b[0m, in \u001b[0;36mTabModel._predict_epoch\u001b[0;34m(self, name, loader)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m# Main loop\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m--> 530\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_batch(X)\n\u001b[1;32m    531\u001b[0m     list_y_true\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m    532\u001b[0m     list_y_score\u001b[39m.\u001b[39mappend(scores)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:558\u001b[0m, in \u001b[0;36mTabModel._predict_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    555\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    557\u001b[0m \u001b[39m# compute model output\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m scores, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scores, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    561\u001b[0m     scores \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m scores]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[0;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:471\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    470\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 471\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    472\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[1;32m    475\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:160\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    158\u001b[0m steps_output \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps):\n\u001b[0;32m--> 160\u001b[0m     M \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_transformers[step](prior, att)\n\u001b[1;32m    161\u001b[0m     M_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\n\u001b[1;32m    162\u001b[0m         torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mmul(M, torch\u001b[39m.\u001b[39mlog(M \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[39m# update prior\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:640\u001b[0m, in \u001b[0;36mAttentiveTransformer.forward\u001b[0;34m(self, priors, processed_feat)\u001b[0m\n\u001b[1;32m    638\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[1;32m    639\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x, priors)\n\u001b[0;32m--> 640\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselector(x)\n\u001b[1;32m    641\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/sparsemax.py:109\u001b[0m, in \u001b[0;36mSparsemax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m sparsemax(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/sparsemax.py:52\u001b[0m, in \u001b[0;36mSparsemaxFunction.forward\u001b[0;34m(ctx, input, dim)\u001b[0m\n\u001b[1;32m     50\u001b[0m max_val, _ \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39mdim, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m \u001b[39minput\u001b[39m \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m max_val  \u001b[39m# same numerical stability trick as for softmax\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m tau, supp_size \u001b[39m=\u001b[39m SparsemaxFunction\u001b[39m.\u001b[39;49m_threshold_and_support(\u001b[39minput\u001b[39;49m, dim\u001b[39m=\u001b[39;49mdim)\n\u001b[1;32m     53\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(\u001b[39minput\u001b[39m \u001b[39m-\u001b[39m tau, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(supp_size, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pytorch_tabnet/sparsemax.py:88\u001b[0m, in \u001b[0;36mSparsemaxFunction._threshold_and_support\u001b[0;34m(input, dim)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_threshold_and_support\u001b[39m(\u001b[39minput\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     71\u001b[0m     \u001b[39m\"\"\"Sparsemax building block: compute the threshold\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     input_srt, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msort(\u001b[39minput\u001b[39;49m, descending\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, dim\u001b[39m=\u001b[39;49mdim)\n\u001b[1;32m     89\u001b[0m     input_cumsum \u001b[39m=\u001b[39m input_srt\u001b[39m.\u001b[39mcumsum(dim) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     90\u001b[0m     rhos \u001b[39m=\u001b[39m _make_ix_like(\u001b[39minput\u001b[39m, dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#make prediction on validation set_clean:\n",
    "\n",
    "\"\"\"\n",
    "The default parameters for TabNet are:\n",
    "    n_D = 8\n",
    "    n_A = 8\n",
    "    n_steps = 3\n",
    "    gamma = 1.3\n",
    "    n_independent = 2\n",
    "    n_shared = 2\n",
    "    epsilon = 1e-15\n",
    "    momentum = 0.3\n",
    "    lambda_sparse = 1e-3\n",
    "    seed = 0\n",
    "    clip_value = 2\n",
    "    optimizer_fn = torch.optim.Adam\n",
    "    optimizer_params = dict(lr=2e-2, weight_decay=1e-5)\n",
    "    mask_type = \"entmax\" # \"sparsemax\"\n",
    "    scheduler_params = dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.9)\n",
    "    scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    verbose = 10\n",
    "    cat_idxs = []\n",
    "    cat_dims = []\n",
    "    cat_emb_dim = 1\n",
    "    device_name = None\n",
    "    \n",
    "\"\"\"\n",
    "mlflow.end_run()\n",
    "mlcbck = MLCallback()\n",
    "MLF_TRACK_URI = \"http://localhost:5000\"\n",
    "MLF_EXP_NAME = \"tabnet_runs\"\n",
    "#make params dict with distribution:\n",
    "params = {\n",
    "    'n_d': scope.int(hp.quniform('n_d', 16, 64, 1)),\n",
    "    'n_a': scope.int(hp.quniform('n_a', 16, 64, 1)),\n",
    "    'n_steps': scope.int(hp.quniform('n_steps', 3, 10, 1)),\n",
    "    'gamma': hp.uniform('gamma', 1.0, 2.0),\n",
    "    'n_independent': scope.int(hp.quniform('n_independent', 1, 5, 1)),\n",
    "    'n_shared': scope.int(hp.quniform('n_shared', 1, 5, 1)),\n",
    "    'lambda_sparse': hp.uniform('lambda_sparse', 1e-5, 1e-1),\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=hp.loguniform('lr', np.log(1e-5), np.log(1e-2))),\n",
    "    'mask_type': hp.choice('mask_type', ['sparsemax', 'entmax'])\n",
    "}\n",
    "\n",
    "#make the objective function:\n",
    "def objective(params):\n",
    "    clf = TabNetClassifier(**params)\n",
    "    clf.fit(\n",
    "        X_train=X_train.values, y_train=y_train.values,\n",
    "        eval_set=[(X_train.values, y_train.values), (X_test.values, y_test.values)],\n",
    "        eval_name=['train', 'test'],\n",
    "        eval_metric=['auc','logloss'],\n",
    "        max_epochs=1000, patience=25,\n",
    "        batch_size=528, virtual_batch_size=64,\n",
    "        num_workers=0,\n",
    "        weights=1,\n",
    "        callbacks=[mlcbck],\n",
    "        drop_last=False\n",
    "    )\n",
    "    y_pred = clf.predict_proba(X_test.values)\n",
    "    #optimize on logloss:\n",
    "    #return the best logloss form all epochs:\n",
    "    return min(clf.history['test_logloss'])\n",
    "    \n",
    "#run the optimization:\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=params,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=Trials(),\n",
    ")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new empty dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "blend_df = pd.DataFrame()\n",
    "#make a prediction on validation_set_f_classif, and add the prediction to the blend_df:\n",
    "y_pred = clf.predict_proba(validation_df_chi2.values)\n",
    "blend_df[\"tabnet_chi2\"] = y_pred[:,1]\n",
    "\n",
    "#load the blend_df:\n",
    "loaded_blend_df = pd.read_csv(\"blend_df.csv\")\n",
    "\n",
    "#add blend_df to loaded_blend_df:\n",
    "loaded_blend_df = pd.concat([loaded_blend_df, blend_df], axis=1)\n",
    "\n",
    "#save to blend_df:\n",
    "loaded_blend_df.to_csv(\"blend_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tabnet_f_classif</th>\n",
       "      <th>tabnet_chi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.625263</td>\n",
       "      <td>0.618708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.735045</td>\n",
       "      <td>0.649769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245663</td>\n",
       "      <td>0.252469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.368006</td>\n",
       "      <td>0.349584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159606</td>\n",
       "      <td>0.124988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.170167</td>\n",
       "      <td>0.202123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0.306913</td>\n",
       "      <td>0.350665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.516423</td>\n",
       "      <td>0.498134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.450064</td>\n",
       "      <td>0.420638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0.318286</td>\n",
       "      <td>0.336678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tabnet_f_classif  tabnet_chi2\n",
       "0              0.625263     0.618708\n",
       "1              0.735045     0.649769\n",
       "2              0.245663     0.252469\n",
       "3              0.368006     0.349584\n",
       "4              0.159606     0.124988\n",
       "...                 ...          ...\n",
       "19995          0.170167     0.202123\n",
       "19996          0.306913     0.350665\n",
       "19997          0.516423     0.498134\n",
       "19998          0.450064     0.420638\n",
       "19999          0.318286     0.336678\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
