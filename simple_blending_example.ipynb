{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "path = Path(os.getcwd())"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-11-01T08:57:08.207311Z",
     "iopub.execute_input": "2022-11-01T08:57:08.207761Z",
     "iopub.status.idle": "2022-11-01T08:57:09.199265Z",
     "shell.execute_reply.started": "2022-11-01T08:57:08.207661Z",
     "shell.execute_reply": "2022-11-01T08:57:09.198213Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/m98612/Projects/Kaggle/Kaggle'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read files and info"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "submission = pd.read_csv(path / 'sample_submissions.csv', index_col='id')\n",
    "labels = pd.read_csv(path / 'train_labels.csv', index_col='id')\n",
    "\n",
    "# the ids of the submission rows (useful later)\n",
    "sub_ids = submission.index\n",
    "\n",
    "# the ids of the labeled rows (useful later)\n",
    "gt_ids = labels.index \n",
    "\n",
    "# list of files in the submission folder\n",
    "subs = sorted(os.listdir(path / 'submission_files'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-01T08:11:01.916605Z",
     "iopub.execute_input": "2022-11-01T08:11:01.916943Z",
     "iopub.status.idle": "2022-11-01T08:11:02.133489Z",
     "shell.execute_reply.started": "2022-11-01T08:11:01.916912Z",
     "shell.execute_reply": "2022-11-01T08:11:02.132615Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read in the first submission file (best scoring on labeled rows)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "s0 = pd.read_csv(path / 'submission_files' / subs[0], index_col='id')\n",
    "\n",
    "score = log_loss(labels, s0.loc[gt_ids])\n",
    "\n",
    "# Notice the score of the labeled rows matches the file name\n",
    "print(subs[0], f'{score:.10f}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-01T08:11:26.392350Z",
     "iopub.execute_input": "2022-11-01T08:11:26.392771Z",
     "iopub.status.idle": "2022-11-01T08:11:26.443519Z",
     "shell.execute_reply.started": "2022-11-01T08:11:26.392737Z",
     "shell.execute_reply": "2022-11-01T08:11:26.442169Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "0.6222863195.csv 0.6222863195\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Same for second submission file"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "s1 = pd.read_csv(path / 'submission_files' / subs[1], index_col='id')\n",
    "\n",
    "score = log_loss(labels, s1.loc[gt_ids])\n",
    "\n",
    "print(subs[1], f'{score:.10f}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-01T08:11:28.053621Z",
     "iopub.execute_input": "2022-11-01T08:11:28.054037Z",
     "iopub.status.idle": "2022-11-01T08:11:28.100339Z",
     "shell.execute_reply.started": "2022-11-01T08:11:28.054000Z",
     "shell.execute_reply": "2022-11-01T08:11:28.099189Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "0.6223807245.csv 0.6223807245\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Blending the two files\n",
    "\n",
    "Blending `s0` and `s1` gives a local score of 0.60497, which is an improvement from the input files (0.622...)\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "blend = (s0 + s1) / 2\n",
    "\n",
    "score = log_loss(labels, blend.loc[gt_ids])\n",
    "\n",
    "print(f'blend score: {score:.10f}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-01T08:11:29.833965Z",
     "iopub.execute_input": "2022-11-01T08:11:29.834897Z",
     "iopub.status.idle": "2022-11-01T08:11:29.851477Z",
     "shell.execute_reply.started": "2022-11-01T08:11:29.834854Z",
     "shell.execute_reply": "2022-11-01T08:11:29.850540Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "blend score: 0.6049715910\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How does the blend do on the Leaderboard?\n",
    "\n",
    "If you submit the unlabeled rows of `s0` (e.g., 20,000 - 39,000) to the leaderboard, it scores 0.61863. Likewise, `s1` scores 0.62335. How does the blend do?\n",
    "\n",
    "#### It scores 0.60454, which is an improvement!"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "blend.loc[sub_ids].to_csv('blend.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-01T03:08:10.803801Z",
     "iopub.execute_input": "2022-11-01T03:08:10.804225Z",
     "iopub.status.idle": "2022-11-01T03:08:10.871312Z",
     "shell.execute_reply.started": "2022-11-01T03:08:10.80419Z",
     "shell.execute_reply": "2022-11-01T03:08:10.870415Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}